{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "from nltk import word_tokenize\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>age</th>\n",
       "      <th>comment</th>\n",
       "      <th>likes/views</th>\n",
       "      <th>link</th>\n",
       "      <th>promo</th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2 DAYS AGO</td>\n",
       "      <td>found my new favorite park!</td>\n",
       "      <td>405,059 likes</td>\n",
       "      <td>/BytNlrQhRx8/</td>\n",
       "      <td>0</td>\n",
       "      <td>chrissyteigen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3 DAYS AGO</td>\n",
       "      <td>Happy bebe!</td>\n",
       "      <td>1,739,218</td>\n",
       "      <td>/Byqz8uZh73s/</td>\n",
       "      <td>0</td>\n",
       "      <td>chrissyteigen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5 DAYS AGO</td>\n",
       "      <td>coated in a paste of fresh garlic and filled w...</td>\n",
       "      <td>2,931,603</td>\n",
       "      <td>/BymErW1B9eL/</td>\n",
       "      <td>0</td>\n",
       "      <td>chrissyteigen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5 DAYS AGO</td>\n",
       "      <td>this kid</td>\n",
       "      <td>371,095 likes</td>\n",
       "      <td>/Byl-aHjBXFX/</td>\n",
       "      <td>0</td>\n",
       "      <td>chrissyteigen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>JUNE 8</td>\n",
       "      <td>home tomorrow üò©</td>\n",
       "      <td>859,039 likes</td>\n",
       "      <td>/ByduG0BB_A3/</td>\n",
       "      <td>0</td>\n",
       "      <td>chrissyteigen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0         age                                            comment  \\\n",
       "0           0  2 DAYS AGO                        found my new favorite park!   \n",
       "1           1  3 DAYS AGO                                        Happy bebe!   \n",
       "2           2  5 DAYS AGO  coated in a paste of fresh garlic and filled w...   \n",
       "3           3  5 DAYS AGO                                           this kid   \n",
       "4           4      JUNE 8                                    home tomorrow üò©   \n",
       "\n",
       "     likes/views           link  promo           user  \n",
       "0  405,059 likes  /BytNlrQhRx8/      0  chrissyteigen  \n",
       "1      1,739,218  /Byqz8uZh73s/      0  chrissyteigen  \n",
       "2      2,931,603  /BymErW1B9eL/      0  chrissyteigen  \n",
       "3  371,095 likes  /Byl-aHjBXFX/      0  chrissyteigen  \n",
       "4  859,039 likes  /ByduG0BB_A3/      0  chrissyteigen  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_df = pd.read_csv('data/posts.csv')\n",
    "posts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1065, 7)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = posts_df.promo\n",
    "data = posts_df['comment'].map(word_tokenize).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8994 unique tokens in our dataset.\n"
     ]
    }
   ],
   "source": [
    "total_vocabulary = set(word for comment in data for word in comment)\n",
    "print(\"There are {} unique tokens in our dataset.\".format(len(total_vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = {}\n",
    "with open('data/glove_data/glove.6B.50d.txt', 'rb') as f:\n",
    "    for line in f:\n",
    "        parts = line.split()\n",
    "        word = parts[0].decode('utf-8')\n",
    "        if word in total_vocabulary:\n",
    "            vector = np.array(parts[1:], dtype=np.float32)\n",
    "            glove[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.65575 ,  0.45659 , -0.16748 , -0.58345 , -0.23073 , -0.78348 ,\n",
       "       -0.23166 , -0.022452, -0.57968 ,  0.526   , -0.2214  ,  0.17614 ,\n",
       "        0.46513 ,  0.79142 ,  0.017403,  1.0879  ,  0.24418 ,  0.27523 ,\n",
       "       -0.26452 , -1.0389  ,  0.014045,  0.68459 ,  0.98151 ,  0.21561 ,\n",
       "        0.36278 , -0.51819 , -0.40552 ,  1.349   ,  1.5399  ,  0.60541 ,\n",
       "        2.6604  ,  0.074535, -0.076292,  0.12501 , -0.026268,  0.16843 ,\n",
       "       -0.41844 ,  0.44505 ,  0.25033 , -1.1557  ,  0.24575 ,  0.41847 ,\n",
       "       -0.10633 , -0.28433 ,  0.51215 ,  0.51371 ,  0.53004 , -0.889   ,\n",
       "        0.054744,  0.78793 ], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove['cool']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Mean Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class W2vVectorizer(object):\n",
    "    \n",
    "    def __init__(self, w2v):\n",
    "        # takes in a dictionary of words and vectors as input\n",
    "        self.w2v = w2v\n",
    "        if len(w2v) == 0:\n",
    "            self.dimensions = 0\n",
    "        else:\n",
    "            self.dimensions = len(w2v[next(iter(glove))])\n",
    "    \n",
    "    # Note from Mike: Even though it doesn't do anything, it's required that this object implement a fit method or else\n",
    "    # It can't be used in a sklearn Pipeline. \n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "            \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.w2v[w] for w in words if w in self.w2v]\n",
    "                   or [np.zeros(self.dimensions)], axis=0) for words in X])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rf =  Pipeline([(\"Word2Vec Vectorizer\", W2vVectorizer(glove)),\n",
    "              (\"Random Forest\", RandomForestClassifier(n_estimators=100, verbose=True))])\n",
    "svc = Pipeline([(\"Word2Vec Vectorizer\", W2vVectorizer(glove)),\n",
    "                ('Support Vector Machine', SVC())])\n",
    "lr = Pipeline([(\"Word2Vec Vectorizer\", W2vVectorizer(glove)),\n",
    "              ('Logistic Regression', LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [('Random Forest', rf),\n",
    "          (\"Support Vector Machine\", svc),\n",
    "          (\"Logistic Regression\", lr)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "/Users/sherzyang/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/sherzyang/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/sherzyang/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/sherzyang/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "scores = [(name, cross_val_score(model, data, target, cv=2).mean()) for name, model, in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Random Forest', 0.6281722128962179),\n",
       " ('Support Vector Machine', 0.5624321121753728),\n",
       " ('Logistic Regression', 0.5877851288634344)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Learning with word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding\n",
    "from tensorflow.keras.layers import Dropout, Activation, Bidirectional, GlobalMaxPool1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from tensorflow.keras.preprocessing import text, sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.get_dummies(target).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer(num_words=20000)\n",
    "tokenizer.fit_on_texts(list(posts_df.comment))\n",
    "list_tokenized_headlines = tokenizer.texts_to_sequences(posts_df.comment)\n",
    "X_t = sequence.pad_sequences(list_tokenized_headlines, maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/sherzyang/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/sherzyang/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 128\n",
    "input_ = Input(shape=(100,))\n",
    "x = Embedding(20000, embedding_size)(input_)\n",
    "x = Bidirectional(LSTM(25, return_sequences=True))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dense(50, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "# There are 2 different possible classes, so we use 2 neurons in our output layer\n",
    "x = Dense(2, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=input_, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 100, 128)          2560000   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 100, 50)           30800     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 2,593,452\n",
      "Trainable params: 2,593,452\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 958 samples, validate on 107 samples\n",
      "WARNING:tensorflow:From /Users/sherzyang/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /Users/sherzyang/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/2\n",
      "958/958 [==============================] - 5s 5ms/sample - loss: 0.6783 - acc: 0.5731 - val_loss: 0.6539 - val_acc: 0.6075\n",
      "Epoch 2/2\n",
      "958/958 [==============================] - 4s 4ms/sample - loss: 0.6431 - acc: 0.6242 - val_loss: 0.6179 - val_acc: 0.6636\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a34ffd240>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_t, y, epochs=2, batch_size=32, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 958 samples, validate on 107 samples\n",
      "Epoch 1/10\n",
      "958/958 [==============================] - 4s 4ms/sample - loss: 0.4844 - acc: 0.8278 - val_loss: 0.5509 - val_acc: 0.7290\n",
      "Epoch 2/10\n",
      "958/958 [==============================] - 3s 4ms/sample - loss: 0.2322 - acc: 0.9363 - val_loss: 0.7209 - val_acc: 0.7196\n",
      "Epoch 3/10\n",
      "958/958 [==============================] - 4s 4ms/sample - loss: 0.1483 - acc: 0.9468 - val_loss: 0.8232 - val_acc: 0.5514\n",
      "Epoch 4/10\n",
      "958/958 [==============================] - 4s 4ms/sample - loss: 0.0867 - acc: 0.9635 - val_loss: 0.8118 - val_acc: 0.6916\n",
      "Epoch 5/10\n",
      "958/958 [==============================] - 4s 4ms/sample - loss: 0.0268 - acc: 0.9979 - val_loss: 0.9954 - val_acc: 0.7009\n",
      "Epoch 6/10\n",
      "958/958 [==============================] - 4s 4ms/sample - loss: 0.0244 - acc: 0.9916 - val_loss: 1.2244 - val_acc: 0.7103\n",
      "Epoch 7/10\n",
      "958/958 [==============================] - 4s 5ms/sample - loss: 0.0111 - acc: 0.9969 - val_loss: 1.3520 - val_acc: 0.6636\n",
      "Epoch 8/10\n",
      "958/958 [==============================] - 4s 4ms/sample - loss: 0.0105 - acc: 0.9979 - val_loss: 1.3742 - val_acc: 0.6822\n",
      "Epoch 9/10\n",
      "958/958 [==============================] - 4s 4ms/sample - loss: 0.0133 - acc: 0.9958 - val_loss: 1.3836 - val_acc: 0.6916\n",
      "Epoch 10/10\n",
      "958/958 [==============================] - 4s 4ms/sample - loss: 0.0054 - acc: 0.9990 - val_loss: 1.4124 - val_acc: 0.6636\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a35070b00>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_t, y, epochs=10, batch_size=32, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "test_df = pickle.load(open('data/testing_posts.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tokenized_headlines = tokenizer.texts_to_sequences(test_df['comment'].map(word_tokenize).values)\n",
    "testing_X = sequence.pad_sequences(list_tokenized_headlines, maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_y = pd.get_dummies(test_df['promo']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248/248 [==============================] - 0s 763us/sample - loss: 1.7697 - acc: 0.6774\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.7696544431870984, 0.67741936]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(testing_X, testing_y, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(testing_X).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>promo</th>\n",
       "      <th>pred_not_promo</th>\n",
       "      <th>pred_promo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Summer is a great time to freshen up the kids‚Äô...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>They all look up to you so much @chipgaines Yo...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Happy Father's Day to the plant daddy of all p...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Y‚Äôall really stepped up to the #ChipInChalleng...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>One more push y‚Äôall! We have til midnight CT t...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  promo  pred_not_promo  \\\n",
       "0  Summer is a great time to freshen up the kids‚Äô...      1            0.01   \n",
       "1  They all look up to you so much @chipgaines Yo...      0            1.00   \n",
       "2  Happy Father's Day to the plant daddy of all p...      0            1.00   \n",
       "3  Y‚Äôall really stepped up to the #ChipInChalleng...      0            0.26   \n",
       "4  One more push y‚Äôall! We have til midnight CT t...      1            0.35   \n",
       "\n",
       "   pred_promo  \n",
       "0        0.99  \n",
       "1        0.00  \n",
       "2        0.00  \n",
       "3        0.74  \n",
       "4        0.65  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df = pd.DataFrame(predictions, columns=['pred_not_promo', 'pred_promo'])\n",
    "simple_test = test_df[['comment', 'promo']]\n",
    "combined_posts_df = pd.concat((simple_test, predictions_df), axis=1)\n",
    "combined_posts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_predicts = []\n",
    "for row in range(len(combined_posts_df)):\n",
    "    pred_promo = combined_posts_df['pred_promo'][row]\n",
    "    pred_not_promo = combined_posts_df['pred_not_promo'][row]\n",
    "    if pred_promo > pred_not_promo:\n",
    "        model_predicts.append(1)\n",
    "    else:\n",
    "        model_predicts.append(0)\n",
    "        \n",
    "model_predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>promo</th>\n",
       "      <th>pred_not_promo</th>\n",
       "      <th>pred_promo</th>\n",
       "      <th>model_predicts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Summer is a great time to freshen up the kids‚Äô...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>They all look up to you so much @chipgaines Yo...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Happy Father's Day to the plant daddy of all p...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Y‚Äôall really stepped up to the #ChipInChalleng...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.74</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>One more push y‚Äôall! We have til midnight CT t...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.65</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  promo  pred_not_promo  \\\n",
       "0  Summer is a great time to freshen up the kids‚Äô...      1            0.01   \n",
       "1  They all look up to you so much @chipgaines Yo...      0            1.00   \n",
       "2  Happy Father's Day to the plant daddy of all p...      0            1.00   \n",
       "3  Y‚Äôall really stepped up to the #ChipInChalleng...      0            0.26   \n",
       "4  One more push y‚Äôall! We have til midnight CT t...      1            0.35   \n",
       "\n",
       "   pred_promo  model_predicts  \n",
       "0        0.99               1  \n",
       "1        0.00               0  \n",
       "2        0.00               0  \n",
       "3        0.74               1  \n",
       "4        0.65               1  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_posts_df['model_predicts'] = model_predicts\n",
    "combined_posts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_pred = []\n",
    "for row in range(len(combined_posts_df)):\n",
    "    true_class = combined_posts_df['promo'][row]\n",
    "    model_pred = combined_posts_df['model_predicts'][row]\n",
    "    if true_class == model_pred:\n",
    "        correct_pred.append(True)\n",
    "    else:\n",
    "        correct_pred.append(False)\n",
    "        \n",
    "correct_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>promo</th>\n",
       "      <th>pred_not_promo</th>\n",
       "      <th>pred_promo</th>\n",
       "      <th>model_predicts</th>\n",
       "      <th>correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Summer is a great time to freshen up the kids‚Äô...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>They all look up to you so much @chipgaines Yo...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Happy Father's Day to the plant daddy of all p...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Y‚Äôall really stepped up to the #ChipInChalleng...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.74</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>One more push y‚Äôall! We have til midnight CT t...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.65</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>After months of developing and finalizing reci...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>When @chipgaines and I got the chance to meet ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Course complete! School looks good on you @chi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.93</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What we witnessed this week at @harvardhbs was...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>16 years and it feels like we're just getting ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  promo  pred_not_promo  \\\n",
       "0  Summer is a great time to freshen up the kids‚Äô...      1            0.01   \n",
       "1  They all look up to you so much @chipgaines Yo...      0            1.00   \n",
       "2  Happy Father's Day to the plant daddy of all p...      0            1.00   \n",
       "3  Y‚Äôall really stepped up to the #ChipInChalleng...      0            0.26   \n",
       "4  One more push y‚Äôall! We have til midnight CT t...      1            0.35   \n",
       "5  After months of developing and finalizing reci...      1            0.00   \n",
       "6  When @chipgaines and I got the chance to meet ...      1            1.00   \n",
       "7  Course complete! School looks good on you @chi...      0            0.07   \n",
       "8  What we witnessed this week at @harvardhbs was...      0            0.00   \n",
       "9  16 years and it feels like we're just getting ...      0            0.10   \n",
       "\n",
       "   pred_promo  model_predicts  correct  \n",
       "0        0.99               1     True  \n",
       "1        0.00               0     True  \n",
       "2        0.00               0     True  \n",
       "3        0.74               1    False  \n",
       "4        0.65               1     True  \n",
       "5        1.00               1     True  \n",
       "6        0.00               0    False  \n",
       "7        0.93               1    False  \n",
       "8        1.00               1    False  \n",
       "9        0.90               1    False  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_posts_df['correct'] = correct_pred\n",
    "combined_posts_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_predictions = combined_posts_df.loc[combined_posts_df['correct'] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Y‚Äôall really stepped up to the #ChipInChallenge and together we raised over $2 million for the fight against cancer! We go to @StJude next week and can‚Äôt wait to see the precious kids and present them with a check for $1.49 million from all of you. On top of that, $512,000 is going to our dear friend Gabe‚Äôs foundation, @bravelikegabe. She will forever inspire us to be brave. ‚ù§Ô∏è Thank you ALL for chipping in with us! #NotTodayCancer',\n",
       "  0),\n",
       " ('When @chipgaines and I got the chance to meet some of the kids at @stjude a couple of years ago, we walked away completely changed. These kids were so full of joy and hope‚ÄîI‚Äôll never forget it. That year, you all helped us raise $230,000 for those kids, and we want to do it again‚Äîbut go even bigger! We‚Äôve pulled a team together to help us with what we‚Äôre calling the #ChipInChallenge, and we hope you‚Äôll be a part of helping us change lives. Head over to the link in profile to find out how you can help.',\n",
       "  1),\n",
       " ('Course complete! School looks good on you @chipgaines ‚ù§Ô∏è #ChipForClassPrez #bems',\n",
       "  0),\n",
       " (\"What we witnessed this week at @harvardhbs was human beings at their very finest. 85 people from all around the world, from all different walks of lives, with different cultural backgrounds, religions, political beliefs and professions. We learned from one another and spoke into each other's lives, sometimes challenging but always valuing every perspective. Not a single one of us were alike, and yet by doing something as simple as listening to and respecting one another, we were challenged and made better. Thankful for this opportunity @anitaelberse ‚Äì your leadership and your passion caused grown, busy adults to think with curiosity about our own businesses and about the world around us. Oh and one more thing... I know I might be biased but I'd like to officially cast my vote for Chip as CLASS PREZ. #ChipForPrez (Photo by @evephoto ) #bems\",\n",
       "  0),\n",
       " (\"16 years and it feels like we're just getting started... I love you Chip Carter Gaines #16\",\n",
       "  0),\n",
       " ('Summer is almost here! ‚ù§Ô∏è', 0),\n",
       " ('She is the most intentional, loyal and fierce woman I know. She is small but mighty and she opens her arms wide for so many. Thank you mom for your extravagant love. I love you‚ù§Ô∏è \"She is clothed with strength and dignity and laughs at the days to come. She speaks with wisdom, and faithful instruction is on her tongue. She watches over the affairs of her household, and does not eat the bread of idleness. Her children arise and call her blessed. Many women do noble things, but you surpass them all.\" Prov 31',\n",
       "  0),\n",
       " (\"We are so thankful for and blown away by each and every runner who participated in this year's #silodistrictmarathon . Watching the grit, the fight, and the spirit of these runners left me speechless. So many of you would tell me who you were running in honor of and I was literally teary eyed the entire time. Thanks for running with us this year- you helped raise $300k for cancer research and the @bravelikegabe foundation. I was also so proud of @chipgaines and Ella (and baby Crew) for running the half marathon today and killing it! Finally, thank you to the city of Waco and all the law enforcement, volunteers and sponsors that helped make this event possible. We hope to see you next year at the #silodistrictmarathon üèÖ\",\n",
       "  0),\n",
       " ('Race weekend is finally here and we are ready! To all the runners and families- Welcome to town! üôåüèΩ#silosdistrictmarathon @magnolia',\n",
       "  0),\n",
       " ('Woke up this morning feeling like it was all a dream. Thank you @time for including us in the #time100. What an honor to be in the room last night‚ù§Ô∏è',\n",
       "  0),\n",
       " ('Thanks @anthonyvaccarello @ysl for having us last night. Performance by @cher was next level epic. @metmuseum',\n",
       "  0),\n",
       " ('@ysl @anthonyvaccarello üì∏ @chriscolls', 1),\n",
       " ('Thanks for the funky shoot @gqaustralia had a great time wearing a colorful array of expensive clothing ;) üì∏ @carterbedloesmith / stylist @atvottero',\n",
       "  1),\n",
       " ('Thanks for the funky shoot @gqaustralia had a great time wearing a colorful array of expensive clothing ;) üì∏ @carterbedloesmith / stylist @atvottero',\n",
       "  1),\n",
       " ('Been messing around with the @redhydrogen camera. Super fun. Check out some of the pics I put on holowpix in 4V. It‚Äôs really spicy üå∂üòé',\n",
       "  0),\n",
       " ('Sorry I couldn‚Äôt make it to the @isntitromantic premier tonight guys. Been dealing with some pretty annoying health stuff the last couple days. Lucky I have the best girl in the world to represent for me! Thanks for the support babe! I hope everyone enjoys the movie! It‚Äôs a perfect Valentine‚Äôs Day flick so if ya ain‚Äôt got nothin better to do then go see it! Love to all!',\n",
       "  1),\n",
       " ('It‚Äôs been a heartbreaking few days. This is what‚Äôs left of my house. Love. Many people in Malibu and surrounding areas in California have lost their homes also and my heart goes out to everyone who was affected by these fires. I spent the day in Malibu yesterday and it was amazing to see the community pulling together to help each other out in any way they can. Malibu is a strong community and this event is only going to make it stronger. Thankful for the all the great local guys that helped keep smaller fires out around my property. I love u guys. I love you Malibu. Thank you to all the hero firefighters around California. It‚Äôs going to be a journey to rebuild. Stay strong all. To help/donate visit @malibufoundation and @happyhippiefdn',\n",
       "  1),\n",
       " ('Happy Halloween! Eat your heart out for the first ever ‚ÄúISN‚ÄôT IT ROMANTIC‚Äù trailer! Valentine‚Äôs Day 2019.',\n",
       "  1),\n",
       " ('‚ÄúThis is my good mood mug‚Äù. Kyle. Arkansas.', 0),\n",
       " ('I live for a queen who writes their elected officials üè≥Ô∏è\\u200düåà love the video & thanks for having us @taylorswift',\n",
       "  0),\n",
       " ('I‚Äôve been thinking about this a lot lately üè≥Ô∏è\\u200düåà Thanks @fransquishco for your time & talent read the full article @outmagazine',\n",
       "  1),\n",
       " ('Thank you so much Chicago üè≥Ô∏è\\u200düåà 2 Sold out shows @chicagotheatre ü§∏üèª\\u200d‚ôÄÔ∏èüò≠üò≠üôèüèªüôèüèª We hope your stomachs are sore from laughing so much @kylejune @jaboukie üí´üí´ Indianapolis can‚Äôt wait for our gorgeous SOLD OUT show tonite!!! üì∏ @andy.argyrakis',\n",
       "  1),\n",
       " ('It‚Äôs #nationalgunviolenceawarenessday & as @speakerpelosi wore orange when I interviewed her last year it‚Äôs the color of #gunviolenceawareness - While these hashtags and days of awareness are important & useful it‚Äôs even better to do something actionable now from this post. The gun lobby @nationalrifleassociation has a death grip on our government and they do so with money. To combat that follow & donate to @momsdemand , @everytown or look up your state legislatures & federal to find out their voting records on gun safety. Our lives have all been touched in someway by gun violence and let‚Äôs wake up & help the people fighting on the front lines. Tag reps fighting for gun safety like @replucymcbath #wearorange',\n",
       "  1),\n",
       " ('I‚Äôm so honored to announce I‚Äôve teamed up with @essie as their first non-female ambassador in celebration of Pride! For me, polish has always been a form of self-expression. Right now that means this mosaic rainbow mani moment. Wearing it proud! üè≥Ô∏è\\u200düåàüíï #essiePartner #essielove #LorealProud',\n",
       "  1),\n",
       " ('Happy Pride loves üíïüíô Time to celebrate with your LGBTQ fam & allies alike to celebrate the diversity that makes everyone stronger! üåàüåàüíï @queereye',\n",
       "  1),\n",
       " (\"Can't think of a better way to spend the day than with these two @themasters. Awesome having them out there with me today #TheMasters\",\n",
       "  0),\n",
       " ('@pumagolf got my threads. Time to focus on my game now. It‚Äôs #TheMasters. Let‚Äôs go!',\n",
       "  1),\n",
       " ('I‚Äôm proud to partner with @CDWcorp and their #TechFore mission to help kids learn valuable life skills through access to golf and technology. #peoplewhogetIT',\n",
       "  1),\n",
       " ('Well it‚Äôs official! Excited to be wearing @pumagolf this year and supporting @volitionameria and @foldsofhonor. I‚Äôll be looking good, feeling good and representing a great cause. #ResistOrdinary',\n",
       "  1),\n",
       " ('Don‚Äôt usually play golf on vacation but @playagrandeclub is so good I had to play a couple times. Luckily @gabby_woodland came to caddie and keep the music going',\n",
       "  1),\n",
       " ('\\u202aLove being back in town for the KU golf reunion. Great time at @lark_a_fare for dinner and drinks. \\u202c',\n",
       "  0),\n",
       " ('Thank you Bellerive Country Club for the hospitality today. Excited to come back next month for the PGA Championship',\n",
       "  0),\n",
       " ('Woke up early to head into Carmel before spending the afternoon at the #USOpen and stopped by Carmel Valley Coffee Roasting Co for some coffees and a beach walk. Now through Sunday (from 6-10am) if you use your @americanexpress Card, you‚Äôll get a complimentary pastry! Another reason to always #shopsmall #amexlife #amexambassador #ad',\n",
       "  1),\n",
       " ('Saturday morning in Carmel üåø #carmelbythesea #summerincalifornia', 0),\n",
       " ('At beautiful Pebble Beach this weekend for the #USOpen thanks to our @americanexpress fam! My golf loving @tberolz is so excited to be here and we love getting to spend some time back in California. If you‚Äôre going to the course this weekend and spend $10 or more on your @AmericanExpress Card at concessions, you‚Äôll get a commemorative cup, which I used all day to stay hydrated on the course. Terms Apply. #AmexLife #AmexAmbassador #Ad',\n",
       "  1),\n",
       " ('Back in one of our favorite places. Next time we‚Äôll be here at the end of the year, we will have our new little travel buddy with us! #asilomarbeach #sunset #california',\n",
       "  0),\n",
       " ('It‚Äôs June, which means baby girl will be here the month after next! Feels like she‚Äôs grown so much over the past week and half üíï #29weeks #babyb #mallorca #spain #babymoon',\n",
       "  0),\n",
       " ('Hi guys I just wanted to share with you my pre and post workout supplements from @myntrx üí™üèº The pre-workout has changed working out for me it gives me so much energy. The BCAA‚Äôs taste so yummy and are so hydrating. Go check out their page and give them a follow!!',\n",
       "  1),\n",
       " ('nothing is better than getting your hair done!!! thank you to @jillenefarrbeauty and @heather.formsalon for making me blonde and happy againüòçüòç',\n",
       "  1),\n",
       " ('After trying my sister @lindsarnold‚Äôs @luftbeds I loved it so much that Topher and I needed to get one for our new apt! A great night sleep is so important to my recovery and fitness routine and @Luftbeds gives me that! You saw my IG story over Presidents Day weekend about #Luft and they let me offer to you all an extension on these awesome deals! So, use discount code JensenQueen ($200 off)  or JensenKing ($300 Off). The mattresses are delivered right to your door! #luftbeds #livelifted #mattress #handmade #madeinusa #luftpartner',\n",
       "  1),\n",
       " ('I need @marisarosemph everyday for my makeupüòç also @aubreebellephotography you‚Äôre the best photographer!! ‚ù§Ô∏è',\n",
       "  1),\n",
       " ('@maglebys_catering is the best you guys!!! If you are looking for a caterer for the big day you need them! The food and desserts were everything that we wanted, and tasted so good! Look how cute the mini donuts areüòç The grilled cheese & tomato soup, and the famous Magleby‚Äôs chocolate cake as our wedding cake made Toph and I soooo happy!‚ù§Ô∏è',\n",
       "  1),\n",
       " ('Our wedding featured on @people üíò link in my bio :) ________________________________________\\n\\nI want to give a HUGE thank you to @pritchettbridal for my dress!! It was everything I wanted and more!! Thank you to @chandikatebeauty for doing my hair and make up for my bridals and wedding day. She is the best and made me feel so beautiful. Also thank you to @pronovias for the cutest fur wrap for my winter wedding!',\n",
       "  1),\n",
       " ('Happy Mother‚Äôs Day to the woman who skipped a grade, set lobsters free in her mother‚Äôs kitchen, stood up to the entire Georgetown Student Senate, became a lawyer, law professor, wrote the Code of Ethics for Child Welfare Professionals for the State of Illinois, rooted for and genuinely believed in the Chicago Bears for four decades, has been married to my Dad for almost 44 years without killing him, has been the greatest mother to all four of us, loving us all the time and being proud of us when we deserved it. Love you Mom. I apologize for stealing money from you occasionally. It was not a lot and stopped many years ago.',\n",
       "  0),\n",
       " ('#tbt May 9 2013. I had the ring in my hand, she turned around, and before I could say ‚Äúwill you..‚Äù she said ‚Äúwait do you have sunscreen on?‚Äù She‚Äôs the fucking best.',\n",
       "  0),\n",
       " ('Thank you @petedavidson for spending your one day off onstage with me every week. You‚Äôre the greatest. Thanks to all of you who came to see us. More dates in the Fall... Photo @marcusrussellprice',\n",
       "  0),\n",
       " ('2015. ‚ÄúThe Devil and Petunia‚Äù also titled ‚ÄúThe Devil and Anthony Jeselnik.‚Äù Anthony‚Äôs new special FIRE IN THE MATERNITY WARD is perfect.\\nIt‚Äôs on @netflixisajoke.',\n",
       "  1),\n",
       " ('Wise Child, Simple Child, Wicked Child and Child Who Doesn‚Äôt Know How to Ask Questions, all roughly 36 years old, looking for and finding the afikoman. Congratulations @odtron!',\n",
       "  0),\n",
       " ('Nice Sweet Boys who lived together (like draculas) in college (in squalor) backstage in Staten Island. Photo by @amtendler',\n",
       "  0),\n",
       " ('Late Shows Added', 1),\n",
       " ('Aight folks we have taken care of the weather! Game on! #indvspak #saifalikhan @theofficialfanatic',\n",
       "  0),\n",
       " (\"This one‚Äôs a winner. Been humming this anthem since I heard it @vivianakadivine. Go all out, team India @virat.kohli. Let's #SockThem! @pumaindia\",\n",
       "  0),\n",
       " ('Miss you already coach! @drewnealpt 9 incredible days of strength training, functional and boxing from the gym to the park! Don‚Äôt get better than that! ü•äüå≥#bodybydrewneal @faroutakhtar RECORD!!!!! üòÇ',\n",
       "  0),\n",
       " ('City lights London nights üì∏ @faroutakhtar #thatbrowngirl', 0),\n",
       " ('CANDY FLOSS üëÖ thank you thank you thank you @shehlaakhan for this custom piece! styled by my rock @khyatibusa makeup by my girl @ashreyaa jewels by @outhousejewellery @curiocottagejewelry',\n",
       "  1),\n",
       " ('@faroutakhtar üç≠\\nthank you thank you @shehlaakhan styled by @khyatibusa\\nhair and makeup by @ashreyaa\\njewels by @outhousejewellery @curiocottagejewelry',\n",
       "  1),\n",
       " ('üç¨\\nDress by the incredible @shehlaakhan love you girl! styled by my girl @khyatibusa makeup by my lovely @ashreyaa üå∏ jewelry by @outhousejewellery\\n@curiocottagejewellery #thatbrowngirl',\n",
       "  1),\n",
       " ('#beachbum #thatbrowngirl\\nbody by @drewnealpt #bodybydrewneal\\nphoto by @faroutakhtar\\noutfit by @hm\\n#nofilter Monday‚Äôs!',\n",
       "  1),\n",
       " ('Imma be right here hanging out!\\n@khyatibusa @sashajairam @anishaachhabriamakeup @reenadutta123 @azima_toppo @aspiringshe\\nKeep following me on @helo.app for all the latest updates!',\n",
       "  1),\n",
       " ('Looking for freedom.... this is where I find it .. with the best coach ever @drewnealpt ü•ä always helps when you punching to the sounds of the queen @beyonce üëë üêù\\n#thatbrowngirl #BodyByDrewNeal',\n",
       "  0),\n",
       " ('Mid week ballin with #dmoney @monicadogra #thatbrowngirl üöÅ üôåüèΩ photo bombing swag @kizeesmack üì∑ @faroutakhtar shades by @rayban_india',\n",
       "  1),\n",
       " ('#PSGIRLS are the happiest @monicadogra @payalsinghal earrings on #thatbrowngirl by @outhousejewellery styled by @khyatibusa üåºüå∏ üì∑ @faroutakhtar üåü',\n",
       "  1),\n",
       " ('If I had an office pup like this I would definitely love going into work everyday! üòù üì¢all job seekers: sign up for the free 5-day challenge and tag a friend who can use help landing their dream job!',\n",
       "  1),\n",
       " ('In 2016, I made it my mission to always feel and embrace feelings of fear because I know that means growth. What I‚Äôve found is that I love fear because once I acknowledge it and push past it anyway, the rewards waiting on the other side make me want to chase after fear ten times over. Today, I‚Äôve now coached hundreds of ambitious corporate professionals to do the same‚Ä¶ scared of networking, scared of presenting, scared of trying‚Ä¶ they too have been able to enjoy the rewards waiting for them on the other side... dream jobs, dream collaborations, dream opportunities, and dream salaries. All from just trying something new! #inspire #cultivateyourcareer',\n",
       "  0),\n",
       " ('It was such an honor speaking on the Womxn in Leadership Panel alongside these amazing career women. We talked everything from conquering failure to defining your own success... to leadership qualities... to mentorship and networking. üå±I often felt so lost navigating my career as a first generation Asian American without many role models. It felt really nice sharing advice I wish I heard early in my career. #mentorship #career #cultivateyourcareer',\n",
       "  0),\n",
       " ('Thank you for all of the birthday love üéÇüéàüéâ\\n„ÄÇ\\nAs I get older, I realize everything I could possibly want, I already have. „ÄÇ\\nThat‚Äôs why this year I want to give back to others who don‚Äôt even have basic needs met. „ÄÇ\\nI‚Äôve decided to make a birthday pledge with @charitywater so millions of people around the globe can have access to clean water and therefore better health, education... everything!\\n„ÄÇ\\nIf you‚Äôre able to donate or simply share the fundraising page (link in bio) it would be the best birthday gift üéÅ Together we can change this world and make it a better place than we found it. „ÄÇ\\nM w a h !',\n",
       "  1),\n",
       " ('Returning to my ‚Äúnew‚Äù normal. .\\n\\nI appreciate the love and support from everyone throughout the month of March. .\\n\\nIt is hard to believe I was in Taiwan for 3 weeks - in my mind it felt more like 3 months. .\\n\\nMany of you shared you‚Äôve also lost a parent throughout the years. I, too, am deeply sorry for your loss. .\\n\\nIt‚Äôs hard to describe this feeling but as I step into April and return back to my ‚Äúnew‚Äù normal I actually feel a deep sensation of acceptance. It‚Äôs as if my dad is actually CLOSER than ever before. As if his spiritual presence is one that I can call upon 24/7. For this, I am so grateful. .\\n\\nI‚Äôm a strong believer in everything happens for a reason and perfectly imperfect. üôèüèº all is good in this universe. Now go hug someone you love and tell them how much they mean to you! ü§ó',\n",
       "  0),\n",
       " ('I talk to so many jobseekers that say something like this:\\n-\\n‚ÄúI‚Äôve been job searching for some time but I haven‚Äôt had any luck. Maybe I should change my profession.‚Äù\\n-\\nWhat they really mean is they‚Äôve been spending hours on their resume, applying online, and sitting back waiting for something to happen... for months. -\\nDon‚Äôt be fooled by applying online. It may feel like you‚Äôre being productive but it is the biggest time waster yet! -\\nYou‚Äôre not hearing anything back because you‚Äôre being passive. -\\nWhile you‚Äôre waiting, a less qualified candidate is messaging the hiring manager directly and asking for an interview. -\\nIt‚Äôs like people waiting in the general admission line at da club... you see that person walking up directly to the bouncer and getting in immediately despite you waiting there for 45 minutes plus and counting. -\\n‚ÄúConventional guys and girls finish last.‚Äù - Emily -\\nI challenge all of my clients to challenge conventional wisdom and do something different from what every other career coach is teaching. üò¨ because I promise you it works!',\n",
       "  0),\n",
       " ('‚ÄúThe unknown carries tremendous opportunities, knowledge, potential, and rewards. Step into it often.‚Äù - Gikandi üå±',\n",
       "  0),\n",
       " ('Aloha! Whenever I travel I am just amazed and in awe at what is created around us. From beautiful nature and all of the variety of plants and animals. To the no two sunsets or skies are the same. To the surfboards that people freaking ride waves on. To the perfect cup that holds my latte. We live in such an inspiring and magnificent world! ‚Ä¢\\nI am in my happy place! The state of being and feeling free, calm, at peace, and completely confident and in control of my future. ‚Ä¢\\nOne of my favorite activities has just been sitting next to the ocean and journaling, reading, and meditating. What‚Äôs amazing is all of this is FREE! Grateful for access to uplifting scenery and letting my imagination run wild. üå±\\n‚Ä¢\\n‚¨áÔ∏èWhat do you do for fun on an üå¥ island? ‚¨áÔ∏è',\n",
       "  0),\n",
       " ('Day 3 of the Dream Candidate Challenge is kicking off in 10 minutes! Weeee!\\nWe‚Äôre talking about the psychology behind the job search and first impressions.\\nI‚Äôm loving the massive action, engagement, connections, and insights ambitious corporate professionals are taking and getting.\\n#grateful',\n",
       "  1),\n",
       " ('I‚Äôve been reflecting on my 2018 and planning my 2019. üå± my mantra this year is going to be ‚ÄúTRUST and THRIVE!‚Äù Trust in myself, the universe/God and thrive to serve as many corporate professionals I can. üíï',\n",
       "  0),\n",
       " ('When you get that ‚Äúyou got RENEWED for SEASON 3‚Äù call!!! Thank you @fxnetworks for giving us a home, for believing in us, for equipping our cast and crew with the resources to make history and move folks to think and do different. Thank you to our loyal viewers for showing up for us and helping us make record numbers for our season 2 premiere (it was our MOST WATCHED EPISODE!) We know you can spend your time anywhere and the fact that you spend it with us means the world to us. We love you!!!! #poseFX',\n",
       "  1),\n",
       " ('Heartbroken and enraged from the harsh reality that Layleen Polanco, who was incarcerated on Rikers Island, was found dead in her cell on Friday. Folks are gathering Monday in NYC and collecting donations to help lay her to rest. Links are in my stories. We need to show up for trans women of color. We need to #CloseRikers. We need to check and challenge any one seeking to police our bodies, genders and identities. Rest in power, Layleen. #closerikersnow #twoc #layleenpolanco #justiceforlayleen #girlslikeus üì∑ @vrye',\n",
       "  0),\n",
       " ('G U C C I', 1),\n",
       " ('Thank you to each and every @televisionacad member for showing up to watch a FYC screening of my directorial debut LOVE IS THE MESSAGE (Pose season 1, episode 6). Thank you to our incomparable cast, writers and producers for making this hour of television possible. It would be a dream to be nominated as a director and a writer ‚Äî the first trans woman of color to do so! ‚Äî and I feel that drama series EMMY nomination for @poseonfx in my bones. Oh, and don‚Äôt forget that @mjrodriguez7 deserves all the üèÜ for carrying our show on her back ‚Äî where would any of us be without Mother Blanca?! Here we stand, together, for your consideration for this year‚Äôs Emmy Awards. #posefx #emmys #fyc',\n",
       "  0),\n",
       " ('The writers room of #PoseFX (missing Ryan and Bradley!) accepting our @televisionacad Honors üèÜ for @poseonfx: ‚ÄúFor its vivid portrayal of a unique subculture, and depiction of such topical issues as LGBTQ prejudice and acceptance, Pose is a worthy recipient.‚Äù',\n",
       "  0),\n",
       " ('V A L E N T I N O', 1),\n",
       " ('I got ready for the #metgala with @elleusa rolling their üé•. It‚Äôs a ki. Get into it!',\n",
       "  1)]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_pred_comments = [(comment, promo) for comment, promo in zip(false_predictions['comment'], false_predictions['promo'])]\n",
    "false_pred_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3185483870967742"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "79/248"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
