{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "from nltk import word_tokenize\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>age</th>\n",
       "      <th>comment</th>\n",
       "      <th>likes/views</th>\n",
       "      <th>link</th>\n",
       "      <th>promo</th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2 DAYS AGO</td>\n",
       "      <td>found my new favorite park!</td>\n",
       "      <td>405,059 likes</td>\n",
       "      <td>/BytNlrQhRx8/</td>\n",
       "      <td>0</td>\n",
       "      <td>chrissyteigen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3 DAYS AGO</td>\n",
       "      <td>Happy bebe!</td>\n",
       "      <td>1,739,218</td>\n",
       "      <td>/Byqz8uZh73s/</td>\n",
       "      <td>0</td>\n",
       "      <td>chrissyteigen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5 DAYS AGO</td>\n",
       "      <td>coated in a paste of fresh garlic and filled w...</td>\n",
       "      <td>2,931,603</td>\n",
       "      <td>/BymErW1B9eL/</td>\n",
       "      <td>0</td>\n",
       "      <td>chrissyteigen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5 DAYS AGO</td>\n",
       "      <td>this kid</td>\n",
       "      <td>371,095 likes</td>\n",
       "      <td>/Byl-aHjBXFX/</td>\n",
       "      <td>0</td>\n",
       "      <td>chrissyteigen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>JUNE 8</td>\n",
       "      <td>home tomorrow üò©</td>\n",
       "      <td>859,039 likes</td>\n",
       "      <td>/ByduG0BB_A3/</td>\n",
       "      <td>0</td>\n",
       "      <td>chrissyteigen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0         age                                            comment  \\\n",
       "0           0  2 DAYS AGO                        found my new favorite park!   \n",
       "1           1  3 DAYS AGO                                        Happy bebe!   \n",
       "2           2  5 DAYS AGO  coated in a paste of fresh garlic and filled w...   \n",
       "3           3  5 DAYS AGO                                           this kid   \n",
       "4           4      JUNE 8                                    home tomorrow üò©   \n",
       "\n",
       "     likes/views           link  promo           user  \n",
       "0  405,059 likes  /BytNlrQhRx8/      0  chrissyteigen  \n",
       "1      1,739,218  /Byqz8uZh73s/      0  chrissyteigen  \n",
       "2      2,931,603  /BymErW1B9eL/      0  chrissyteigen  \n",
       "3  371,095 likes  /Byl-aHjBXFX/      0  chrissyteigen  \n",
       "4  859,039 likes  /ByduG0BB_A3/      0  chrissyteigen  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_df = pd.read_csv('data/posts.csv')\n",
    "posts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1065, 7)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = posts_df.promo\n",
    "data = posts_df['comment'].map(word_tokenize).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8994 unique tokens in our dataset.\n"
     ]
    }
   ],
   "source": [
    "total_vocabulary = set(word for comment in data for word in comment)\n",
    "print(\"There are {} unique tokens in our dataset.\".format(len(total_vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = {}\n",
    "with open('data/glove_data/glove.6B.50d.txt', 'rb') as f:\n",
    "    for line in f:\n",
    "        parts = line.split()\n",
    "        word = parts[0].decode('utf-8')\n",
    "        if word in total_vocabulary:\n",
    "            vector = np.array(parts[1:], dtype=np.float32)\n",
    "            glove[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.65575 ,  0.45659 , -0.16748 , -0.58345 , -0.23073 , -0.78348 ,\n",
       "       -0.23166 , -0.022452, -0.57968 ,  0.526   , -0.2214  ,  0.17614 ,\n",
       "        0.46513 ,  0.79142 ,  0.017403,  1.0879  ,  0.24418 ,  0.27523 ,\n",
       "       -0.26452 , -1.0389  ,  0.014045,  0.68459 ,  0.98151 ,  0.21561 ,\n",
       "        0.36278 , -0.51819 , -0.40552 ,  1.349   ,  1.5399  ,  0.60541 ,\n",
       "        2.6604  ,  0.074535, -0.076292,  0.12501 , -0.026268,  0.16843 ,\n",
       "       -0.41844 ,  0.44505 ,  0.25033 , -1.1557  ,  0.24575 ,  0.41847 ,\n",
       "       -0.10633 , -0.28433 ,  0.51215 ,  0.51371 ,  0.53004 , -0.889   ,\n",
       "        0.054744,  0.78793 ], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove['cool']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Mean Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class W2vVectorizer(object):\n",
    "    \n",
    "    def __init__(self, w2v):\n",
    "        # takes in a dictionary of words and vectors as input\n",
    "        self.w2v = w2v\n",
    "        if len(w2v) == 0:\n",
    "            self.dimensions = 0\n",
    "        else:\n",
    "            self.dimensions = len(w2v[next(iter(glove))])\n",
    "    \n",
    "    # Note from Mike: Even though it doesn't do anything, it's required that this object implement a fit method or else\n",
    "    # It can't be used in a sklearn Pipeline. \n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "            \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.w2v[w] for w in words if w in self.w2v]\n",
    "                   or [np.zeros(self.dimensions)], axis=0) for words in X])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rf =  Pipeline([(\"Word2Vec Vectorizer\", W2vVectorizer(glove)),\n",
    "              (\"Random Forest\", RandomForestClassifier(n_estimators=100, verbose=True))])\n",
    "svc = Pipeline([(\"Word2Vec Vectorizer\", W2vVectorizer(glove)),\n",
    "                ('Support Vector Machine', SVC())])\n",
    "lr = Pipeline([(\"Word2Vec Vectorizer\", W2vVectorizer(glove)),\n",
    "              ('Logistic Regression', LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [('Random Forest', rf),\n",
    "          (\"Support Vector Machine\", svc),\n",
    "          (\"Logistic Regression\", lr)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "/Users/sherzyang/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/sherzyang/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/sherzyang/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/sherzyang/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "scores = [(name, cross_val_score(model, data, target, cv=2).mean()) for name, model, in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Random Forest', 0.6281722128962179),\n",
       " ('Support Vector Machine', 0.5624321121753728),\n",
       " ('Logistic Regression', 0.5877851288634344)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Learning with word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding\n",
    "from tensorflow.keras.layers import Dropout, Activation, Bidirectional, GlobalMaxPool1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from tensorflow.keras.preprocessing import text, sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.get_dummies(target).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer(num_words=20000)\n",
    "tokenizer.fit_on_texts(list(posts_df.comment))\n",
    "list_tokenized_headlines = tokenizer.texts_to_sequences(posts_df.comment)\n",
    "X_t = sequence.pad_sequences(list_tokenized_headlines, maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/sherzyang/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/sherzyang/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 128\n",
    "input_ = Input(shape=(100,))\n",
    "x = Embedding(20000, embedding_size)(input_)\n",
    "x = Bidirectional(LSTM(25, return_sequences=True))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dense(50, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "# There are 2 different possible classes, so we use 2 neurons in our output layer\n",
    "x = Dense(2, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=input_, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 100, 128)          2560000   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 100, 50)           30800     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 2,593,452\n",
      "Trainable params: 2,593,452\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 958 samples, validate on 107 samples\n",
      "WARNING:tensorflow:From /Users/sherzyang/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /Users/sherzyang/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/2\n",
      "958/958 [==============================] - 5s 6ms/sample - loss: 0.6828 - acc: 0.5637 - val_loss: 0.6577 - val_acc: 0.6075\n",
      "Epoch 2/2\n",
      "958/958 [==============================] - 4s 4ms/sample - loss: 0.6414 - acc: 0.6597 - val_loss: 0.6218 - val_acc: 0.7009\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a3d78a4a8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_t, y, epochs=2, batch_size=32, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 958 samples, validate on 107 samples\n",
      "Epoch 1/10\n",
      "958/958 [==============================] - 4s 4ms/sample - loss: 0.4796 - acc: 0.8225 - val_loss: 0.5624 - val_acc: 0.7664\n",
      "Epoch 2/10\n",
      "958/958 [==============================] - 4s 4ms/sample - loss: 0.1986 - acc: 0.9426 - val_loss: 0.5744 - val_acc: 0.7664\n",
      "Epoch 3/10\n",
      "958/958 [==============================] - 4s 4ms/sample - loss: 0.0678 - acc: 0.9802 - val_loss: 0.7100 - val_acc: 0.7850\n",
      "Epoch 4/10\n",
      "958/958 [==============================] - 4s 4ms/sample - loss: 0.0215 - acc: 0.9958 - val_loss: 0.8397 - val_acc: 0.7477\n",
      "Epoch 5/10\n",
      "958/958 [==============================] - 4s 4ms/sample - loss: 0.0106 - acc: 0.9979 - val_loss: 0.9426 - val_acc: 0.7477\n",
      "Epoch 6/10\n",
      "958/958 [==============================] - 4s 4ms/sample - loss: 0.0055 - acc: 0.9990 - val_loss: 1.2368 - val_acc: 0.7477\n",
      "Epoch 7/10\n",
      "958/958 [==============================] - 4s 4ms/sample - loss: 0.0070 - acc: 0.9979 - val_loss: 1.1878 - val_acc: 0.7103\n",
      "Epoch 8/10\n",
      "958/958 [==============================] - 4s 4ms/sample - loss: 0.0029 - acc: 1.0000 - val_loss: 1.5399 - val_acc: 0.7290\n",
      "Epoch 9/10\n",
      "958/958 [==============================] - 4s 4ms/sample - loss: 0.0012 - acc: 1.0000 - val_loss: 1.6687 - val_acc: 0.7103\n",
      "Epoch 10/10\n",
      "958/958 [==============================] - 4s 4ms/sample - loss: 0.0049 - acc: 0.9969 - val_loss: 1.2497 - val_acc: 0.7290\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a3d8164a8>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_t, y, epochs=10, batch_size=32, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "test_df = pickle.load(open('data/testing_posts.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tokenized_headlines = tokenizer.texts_to_sequences(test_df['comment'].map(word_tokenize).values)\n",
    "testing_X = sequence.pad_sequences(list_tokenized_headlines, maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_y = pd.get_dummies(test_df['promo']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248/248 [==============================] - 0s 822us/sample - loss: 1.6922 - acc: 0.6935\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.6922296247174662, 0.6935484]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(testing_X, testing_y, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(testing_X).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>promo</th>\n",
       "      <th>pred_not_promo</th>\n",
       "      <th>pred_promo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Summer is a great time to freshen up the kids‚Äô...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>They all look up to you so much @chipgaines Yo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Happy Father's Day to the plant daddy of all p...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Y‚Äôall really stepped up to the #ChipInChalleng...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>One more push y‚Äôall! We have til midnight CT t...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  promo  pred_not_promo  \\\n",
       "0  Summer is a great time to freshen up the kids‚Äô...      1            0.00   \n",
       "1  They all look up to you so much @chipgaines Yo...      0            0.15   \n",
       "2  Happy Father's Day to the plant daddy of all p...      0            0.99   \n",
       "3  Y‚Äôall really stepped up to the #ChipInChalleng...      0            0.00   \n",
       "4  One more push y‚Äôall! We have til midnight CT t...      1            0.00   \n",
       "\n",
       "   pred_promo  \n",
       "0        1.00  \n",
       "1        0.85  \n",
       "2        0.01  \n",
       "3        1.00  \n",
       "4        1.00  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df = pd.DataFrame(predictions, columns=['pred_not_promo', 'pred_promo'])\n",
    "simple_test = test_df[['comment', 'promo']]\n",
    "combined_posts_df = pd.concat((simple_test, predictions_df), axis=1)\n",
    "combined_posts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_predicts = []\n",
    "for row in range(len(combined_posts_df)):\n",
    "    pred_promo = combined_posts_df['pred_promo'][row]\n",
    "    pred_not_promo = combined_posts_df['pred_not_promo'][row]\n",
    "    if pred_promo > pred_not_promo:\n",
    "        model_predicts.append(1)\n",
    "    else:\n",
    "        model_predicts.append(0)\n",
    "        \n",
    "model_predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>promo</th>\n",
       "      <th>pred_not_promo</th>\n",
       "      <th>pred_promo</th>\n",
       "      <th>model_predicts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Summer is a great time to freshen up the kids‚Äô...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>They all look up to you so much @chipgaines Yo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.85</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Happy Father's Day to the plant daddy of all p...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Y‚Äôall really stepped up to the #ChipInChalleng...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>One more push y‚Äôall! We have til midnight CT t...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  promo  pred_not_promo  \\\n",
       "0  Summer is a great time to freshen up the kids‚Äô...      1            0.00   \n",
       "1  They all look up to you so much @chipgaines Yo...      0            0.15   \n",
       "2  Happy Father's Day to the plant daddy of all p...      0            0.99   \n",
       "3  Y‚Äôall really stepped up to the #ChipInChalleng...      0            0.00   \n",
       "4  One more push y‚Äôall! We have til midnight CT t...      1            0.00   \n",
       "\n",
       "   pred_promo  model_predicts  \n",
       "0        1.00               1  \n",
       "1        0.85               1  \n",
       "2        0.01               0  \n",
       "3        1.00               1  \n",
       "4        1.00               1  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_posts_df['model_predicts'] = model_predicts\n",
    "combined_posts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_pred = []\n",
    "for row in range(len(combined_posts_df)):\n",
    "    true_class = combined_posts_df['promo'][row]\n",
    "    model_pred = combined_posts_df['model_predicts'][row]\n",
    "    if true_class == model_pred:\n",
    "        correct_pred.append(True)\n",
    "    else:\n",
    "        correct_pred.append(False)\n",
    "        \n",
    "correct_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>promo</th>\n",
       "      <th>pred_not_promo</th>\n",
       "      <th>pred_promo</th>\n",
       "      <th>model_predicts</th>\n",
       "      <th>correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Summer is a great time to freshen up the kids‚Äô...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>They all look up to you so much @chipgaines Yo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.85</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Happy Father's Day to the plant daddy of all p...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Y‚Äôall really stepped up to the #ChipInChalleng...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>One more push y‚Äôall! We have til midnight CT t...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>After months of developing and finalizing reci...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>When @chipgaines and I got the chance to meet ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Course complete! School looks good on you @chi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What we witnessed this week at @harvardhbs was...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>16 years and it feels like we're just getting ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  promo  pred_not_promo  \\\n",
       "0  Summer is a great time to freshen up the kids‚Äô...      1            0.00   \n",
       "1  They all look up to you so much @chipgaines Yo...      0            0.15   \n",
       "2  Happy Father's Day to the plant daddy of all p...      0            0.99   \n",
       "3  Y‚Äôall really stepped up to the #ChipInChalleng...      0            0.00   \n",
       "4  One more push y‚Äôall! We have til midnight CT t...      1            0.00   \n",
       "5  After months of developing and finalizing reci...      1            0.00   \n",
       "6  When @chipgaines and I got the chance to meet ...      1            0.97   \n",
       "7  Course complete! School looks good on you @chi...      0            0.54   \n",
       "8  What we witnessed this week at @harvardhbs was...      0            0.00   \n",
       "9  16 years and it feels like we're just getting ...      0            0.25   \n",
       "\n",
       "   pred_promo  model_predicts  correct  \n",
       "0        1.00               1     True  \n",
       "1        0.85               1    False  \n",
       "2        0.01               0     True  \n",
       "3        1.00               1    False  \n",
       "4        1.00               1     True  \n",
       "5        1.00               1     True  \n",
       "6        0.03               0    False  \n",
       "7        0.46               0     True  \n",
       "8        1.00               1    False  \n",
       "9        0.75               1    False  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_posts_df['correct'] = correct_pred\n",
    "combined_posts_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_predictions = combined_posts_df.loc[combined_posts_df['correct'] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"They all look up to you so much @chipgaines You're the strongest, bravest, funniest, and most loving man they know. You lead our babies well- Happy Father's Day! ‚ù§Ô∏è\",\n",
       "  0),\n",
       " ('Y‚Äôall really stepped up to the #ChipInChallenge and together we raised over $2 million for the fight against cancer! We go to @StJude next week and can‚Äôt wait to see the precious kids and present them with a check for $1.49 million from all of you. On top of that, $512,000 is going to our dear friend Gabe‚Äôs foundation, @bravelikegabe. She will forever inspire us to be brave. ‚ù§Ô∏è Thank you ALL for chipping in with us! #NotTodayCancer',\n",
       "  0),\n",
       " ('When @chipgaines and I got the chance to meet some of the kids at @stjude a couple of years ago, we walked away completely changed. These kids were so full of joy and hope‚ÄîI‚Äôll never forget it. That year, you all helped us raise $230,000 for those kids, and we want to do it again‚Äîbut go even bigger! We‚Äôve pulled a team together to help us with what we‚Äôre calling the #ChipInChallenge, and we hope you‚Äôll be a part of helping us change lives. Head over to the link in profile to find out how you can help.',\n",
       "  1),\n",
       " (\"What we witnessed this week at @harvardhbs was human beings at their very finest. 85 people from all around the world, from all different walks of lives, with different cultural backgrounds, religions, political beliefs and professions. We learned from one another and spoke into each other's lives, sometimes challenging but always valuing every perspective. Not a single one of us were alike, and yet by doing something as simple as listening to and respecting one another, we were challenged and made better. Thankful for this opportunity @anitaelberse ‚Äì your leadership and your passion caused grown, busy adults to think with curiosity about our own businesses and about the world around us. Oh and one more thing... I know I might be biased but I'd like to officially cast my vote for Chip as CLASS PREZ. #ChipForPrez (Photo by @evephoto ) #bems\",\n",
       "  0),\n",
       " (\"16 years and it feels like we're just getting started... I love you Chip Carter Gaines #16\",\n",
       "  0),\n",
       " ('Summer is almost here! ‚ù§Ô∏è', 0),\n",
       " ('She is the most intentional, loyal and fierce woman I know. She is small but mighty and she opens her arms wide for so many. Thank you mom for your extravagant love. I love you‚ù§Ô∏è \"She is clothed with strength and dignity and laughs at the days to come. She speaks with wisdom, and faithful instruction is on her tongue. She watches over the affairs of her household, and does not eat the bread of idleness. Her children arise and call her blessed. Many women do noble things, but you surpass them all.\" Prov 31',\n",
       "  0),\n",
       " ('Race weekend is finally here and we are ready! To all the runners and families- Welcome to town! üôåüèΩ#silosdistrictmarathon @magnolia',\n",
       "  0),\n",
       " ('Woke up this morning feeling like it was all a dream. Thank you @time for including us in the #time100. What an honor to be in the room last night‚ù§Ô∏è',\n",
       "  0),\n",
       " ('Thanks @anthonyvaccarello @ysl for having us last night. Performance by @cher was next level epic. @metmuseum',\n",
       "  0),\n",
       " ('@ysl @anthonyvaccarello üì∏ @chriscolls', 1),\n",
       " ('Been messing around with the @redhydrogen camera. Super fun. Check out some of the pics I put on holowpix in 4V. It‚Äôs really spicy üå∂üòé',\n",
       "  0),\n",
       " ('Sorry I couldn‚Äôt make it to the @isntitromantic premier tonight guys. Been dealing with some pretty annoying health stuff the last couple days. Lucky I have the best girl in the world to represent for me! Thanks for the support babe! I hope everyone enjoys the movie! It‚Äôs a perfect Valentine‚Äôs Day flick so if ya ain‚Äôt got nothin better to do then go see it! Love to all!',\n",
       "  1),\n",
       " ('It‚Äôs been a heartbreaking few days. This is what‚Äôs left of my house. Love. Many people in Malibu and surrounding areas in California have lost their homes also and my heart goes out to everyone who was affected by these fires. I spent the day in Malibu yesterday and it was amazing to see the community pulling together to help each other out in any way they can. Malibu is a strong community and this event is only going to make it stronger. Thankful for the all the great local guys that helped keep smaller fires out around my property. I love u guys. I love you Malibu. Thank you to all the hero firefighters around California. It‚Äôs going to be a journey to rebuild. Stay strong all. To help/donate visit @malibufoundation and @happyhippiefdn',\n",
       "  1),\n",
       " ('Happy Halloween! Eat your heart out for the first ever ‚ÄúISN‚ÄôT IT ROMANTIC‚Äù trailer! Valentine‚Äôs Day 2019.',\n",
       "  1),\n",
       " ('I live for a queen who writes their elected officials üè≥Ô∏è\\u200düåà love the video & thanks for having us @taylorswift',\n",
       "  0),\n",
       " ('Last June 4th, the Supreme Court ruled in favor of a cake shop owner who chose not to bake a wedding cake for a same-sex couple. On the one year anniversary of the reversal, I was honored to partner with @elysianbrewing to officiate a very special wedding. Many happy tears were shed as Megan @meganberrycreative & Haden @bart_arts tied the knot and we cut into six epic cakes alongside the court case couple, Charlie @charliewcraig & David @moofian2 . Cheers to marriage equality and happily ever after for everyone. #MarryUsJVN thanks so much @elysianbrewing for making such a beautiful and celebratory day #pride #ad ‚Äî A note on this ruling - it‚Äôs important to clarify this Supreme Court decision was narrow & does not establish precedent for LGBTQ discrimination in services nationally but rather it specifically reversed the ruling in this case for extenuating circumstances.',\n",
       "  1),\n",
       " ('I‚Äôve been thinking about this a lot lately üè≥Ô∏è\\u200düåà Thanks @fransquishco for your time & talent read the full article @outmagazine',\n",
       "  1),\n",
       " ('Greetings from my sold out #roadtobeijing show in Indianapolis last night #2020 üè≥Ô∏è\\u200düåà Religious Liberties = Right to Discriminate @vp',\n",
       "  0),\n",
       " ('It‚Äôs #nationalgunviolenceawarenessday & as @speakerpelosi wore orange when I interviewed her last year it‚Äôs the color of #gunviolenceawareness - While these hashtags and days of awareness are important & useful it‚Äôs even better to do something actionable now from this post. The gun lobby @nationalrifleassociation has a death grip on our government and they do so with money. To combat that follow & donate to @momsdemand , @everytown or look up your state legislatures & federal to find out their voting records on gun safety. Our lives have all been touched in someway by gun violence and let‚Äôs wake up & help the people fighting on the front lines. Tag reps fighting for gun safety like @replucymcbath #wearorange',\n",
       "  1),\n",
       " ('I‚Äôm so honored to announce I‚Äôve teamed up with @essie as their first non-female ambassador in celebration of Pride! For me, polish has always been a form of self-expression. Right now that means this mosaic rainbow mani moment. Wearing it proud! üè≥Ô∏è\\u200düåàüíï #essiePartner #essielove #LorealProud',\n",
       "  1),\n",
       " ('Happy Pride loves üíïüíô Time to celebrate with your LGBTQ fam & allies alike to celebrate the diversity that makes everyone stronger! üåàüåàüíï @queereye',\n",
       "  1),\n",
       " ('@pumagolf got my threads. Time to focus on my game now. It‚Äôs #TheMasters. Let‚Äôs go!',\n",
       "  1),\n",
       " ('I‚Äôm proud to partner with @CDWcorp and their #TechFore mission to help kids learn valuable life skills through access to golf and technology. #peoplewhogetIT',\n",
       "  1),\n",
       " ('Well it‚Äôs official! Excited to be wearing @pumagolf this year and supporting @volitionameria and @foldsofhonor. I‚Äôll be looking good, feeling good and representing a great cause. #ResistOrdinary',\n",
       "  1),\n",
       " ('I‚Äôm extremely excited to be joining @WilsonGolf after working with them extensively the last few months. Their premium line of clubs and high-quality equipment will be integral as I continue to elevate my game. #WilsonStaff',\n",
       "  1),\n",
       " ('Don‚Äôt usually play golf on vacation but @playagrandeclub is so good I had to play a couple times. Luckily @gabby_woodland came to caddie and keep the music going',\n",
       "  1),\n",
       " ('\\u202aLove being back in town for the KU golf reunion. Great time at @lark_a_fare for dinner and drinks. \\u202c',\n",
       "  0),\n",
       " ('Thank you Bellerive Country Club for the hospitality today. Excited to come back next month for the PGA Championship',\n",
       "  0),\n",
       " ('Little fine tuning with Butch before playing 3 weeks in a row', 1),\n",
       " ('Truly an incredible experience this week at the @wmphoenixopen. Thank you to the tournament for once again putting on one of the best events all year, the fans for creating an atmosphere that is unmatched, my partners @uagolf, @CDWcorp, @taylormadegolf, and @netjets for the endless support, and my family for waiting for me as I walked off 18.',\n",
       "  0),\n",
       " ('They say marriage is about compromising so I‚Äôm glad @gabby_woodland realizes we only root for Kansas even though she went to Baylor',\n",
       "  0),\n",
       " ('Back in one of our favorite places. Next time we‚Äôll be here at the end of the year, we will have our new little travel buddy with us! #asilomarbeach #sunset #california',\n",
       "  0),\n",
       " ('A little over 30 weeks with this bump and it finally feels like we‚Äôre getting so close to meeting her! We‚Äôre on our last leg of travel (for my sister @laurentaylorcreates wedding in Ireland!) before we‚Äôre home bound for baby. More over on galmeetsglam.com today #30weeks #babyb #spain #mallorca',\n",
       "  1),\n",
       " ('Picking up things for Baby B in the small charming town of Valldemossa. Love seeking out local shops and shopping small with @AmericanExpress to support small businesses at home and abroad. I can‚Äôt wait to see the little lady in the dress we got her! #ShopSmall #AmexAmbassador #AmexLife #gmgcollection #gmgonme',\n",
       "  1),\n",
       " ('Roses and lemons everywhere üçãüå∏ The most amazing scene and smell to arrive to #mallorca #spain #roses #lemons #summercolors #babymoon',\n",
       "  0),\n",
       " ('It‚Äôs June, which means baby girl will be here the month after next! Feels like she‚Äôs grown so much over the past week and half üíï #29weeks #babyb #mallorca #spain #babymoon',\n",
       "  0),\n",
       " ('GIVEWAY IS CLOSED!! congrats to @hannahhbooren I will DM you!\\nI have teamed up with @furbodogcamera to give ONE lucky winner this amazing #dogcamera üê∂ We LOVE our new doggie camera so much! It has been super hard for us to leave Ace alone. But now with the camera, we can see what he‚Äôs up to, and throw him some treats!! #furbotreattime To enter complete the following steps‚¨áÔ∏è\\n1. Follow @furbodogcamera and @jensenarnold_\\n2. Like this photo‚ù§Ô∏è 3. Tag your friends!! Each comment will be counted as one entryüòä Good luck everyone! Winner will be announced 3/31/19! #furbo #caughtoncamera #ad',\n",
       "  1),\n",
       " ('Hi guys I just wanted to share with you my pre and post workout supplements from @myntrx üí™üèº The pre-workout has changed working out for me it gives me so much energy. The BCAA‚Äôs taste so yummy and are so hydrating. Go check out their page and give them a follow!!',\n",
       "  1),\n",
       " ('After trying my sister @lindsarnold‚Äôs @luftbeds I loved it so much that Topher and I needed to get one for our new apt! A great night sleep is so important to my recovery and fitness routine and @Luftbeds gives me that! You saw my IG story over Presidents Day weekend about #Luft and they let me offer to you all an extension on these awesome deals! So, use discount code JensenQueen ($200 off)  or JensenKing ($300 Off). The mattresses are delivered right to your door! #luftbeds #livelifted #mattress #handmade #madeinusa #luftpartner',\n",
       "  1),\n",
       " ('I need @marisarosemph everyday for my makeupüòç also @aubreebellephotography you‚Äôre the best photographer!! ‚ù§Ô∏è',\n",
       "  1),\n",
       " ('@maglebys_catering is the best you guys!!! If you are looking for a caterer for the big day you need them! The food and desserts were everything that we wanted, and tasted so good! Look how cute the mini donuts areüòç The grilled cheese & tomato soup, and the famous Magleby‚Äôs chocolate cake as our wedding cake made Toph and I soooo happy!‚ù§Ô∏è',\n",
       "  1),\n",
       " ('As I once said to @amtendler ‚Äúyou are an art project that is always evolving. I am like...a benign cartoon character.‚Äù This profile in @nylonmag captures the first part of that sentiment. Reading this just reminds me how amazing and lucky it is to watch this woman create every day. Lincoln bio.',\n",
       "  1),\n",
       " ('Root Canal is fillers at half the price. Why waste a buck? #lifehack #lol',\n",
       "  0),\n",
       " ('Happy Mother‚Äôs Day to the woman who skipped a grade, set lobsters free in her mother‚Äôs kitchen, stood up to the entire Georgetown Student Senate, became a lawyer, law professor, wrote the Code of Ethics for Child Welfare Professionals for the State of Illinois, rooted for and genuinely believed in the Chicago Bears for four decades, has been married to my Dad for almost 44 years without killing him, has been the greatest mother to all four of us, loving us all the time and being proud of us when we deserved it. Love you Mom. I apologize for stealing money from you occasionally. It was not a lot and stopped many years ago.',\n",
       "  0),\n",
       " ('my dog @laviepetunia has a new hairpiece that brings her a lot of confidence.',\n",
       "  0),\n",
       " ('#tbt May 9 2013. I had the ring in my hand, she turned around, and before I could say ‚Äúwill you..‚Äù she said ‚Äúwait do you have sunscreen on?‚Äù She‚Äôs the fucking best.',\n",
       "  0),\n",
       " ('Thank you @petedavidson for spending your one day off onstage with me every week. You‚Äôre the greatest. Thanks to all of you who came to see us. More dates in the Fall... Photo @marcusrussellprice',\n",
       "  0),\n",
       " ('2015. ‚ÄúThe Devil and Petunia‚Äù also titled ‚ÄúThe Devil and Anthony Jeselnik.‚Äù Anthony‚Äôs new special FIRE IN THE MATERNITY WARD is perfect.\\nIt‚Äôs on @netflixisajoke.',\n",
       "  1),\n",
       " ('Wise Child, Simple Child, Wicked Child and Child Who Doesn‚Äôt Know How to Ask Questions, all roughly 36 years old, looking for and finding the afikoman. Congratulations @odtron!',\n",
       "  0),\n",
       " ('Late Shows Added', 1),\n",
       " ('Aight folks we have taken care of the weather! Game on! #indvspak #saifalikhan @theofficialfanatic',\n",
       "  0),\n",
       " ('@zimmermann @sacaiofficial @burberry styled by @khyatibusa hair @reenadutta123 makeup @ritarathod786 #Repost @gqindia\\n„Éª„Éª„Éª\\n#GQBestDressed 100: Potent, savvy dressers in this trio: Dior rep @anuragty, @shibanidandekar and musician @zaedenmusic. #June2019 Photo: Manasi Sawant\\n\\n_________________________________________\\n\\n#BestDressed #StyleFlex #Fashion #ShibaniDandekar #Dior #AnuragTyagi #Zaeden #SahilSharma #Musician #GQShoot',\n",
       "  1),\n",
       " ('Miss you already coach! @drewnealpt 9 incredible days of strength training, functional and boxing from the gym to the park! Don‚Äôt get better than that! ü•äüå≥#bodybydrewneal @faroutakhtar RECORD!!!!! üòÇ',\n",
       "  0),\n",
       " ('@faroutakhtar üç≠\\nthank you thank you @shehlaakhan styled by @khyatibusa\\nhair and makeup by @ashreyaa\\njewels by @outhousejewellery @curiocottagejewelry',\n",
       "  1),\n",
       " ('üç¨\\nDress by the incredible @shehlaakhan love you girl! styled by my girl @khyatibusa makeup by my lovely @ashreyaa üå∏ jewelry by @outhousejewellery\\n@curiocottagejewellery #thatbrowngirl',\n",
       "  1),\n",
       " ('#beachbum #thatbrowngirl\\nbody by @drewnealpt #bodybydrewneal\\nphoto by @faroutakhtar\\noutfit by @hm\\n#nofilter Monday‚Äôs!',\n",
       "  1),\n",
       " ('Imma be right here hanging out!\\n@khyatibusa @sashajairam @anishaachhabriamakeup @reenadutta123 @azima_toppo @aspiringshe\\nKeep following me on @helo.app for all the latest updates!',\n",
       "  1),\n",
       " ('Looking for freedom.... this is where I find it .. with the best coach ever @drewnealpt ü•ä always helps when you punching to the sounds of the queen @beyonce üëë üêù\\n#thatbrowngirl #BodyByDrewNeal',\n",
       "  0),\n",
       " ('Mid week ballin with #dmoney @monicadogra #thatbrowngirl üöÅ üôåüèΩ photo bombing swag @kizeesmack üì∑ @faroutakhtar shades by @rayban_india',\n",
       "  1),\n",
       " ('#PSGIRLS are the happiest @monicadogra @payalsinghal earrings on #thatbrowngirl by @outhousejewellery styled by @khyatibusa üåºüå∏ üì∑ @faroutakhtar üåü',\n",
       "  1),\n",
       " ('If I had an office pup like this I would definitely love going into work everyday! üòù üì¢all job seekers: sign up for the free 5-day challenge and tag a friend who can use help landing their dream job!',\n",
       "  1),\n",
       " ('In 2016, I made it my mission to always feel and embrace feelings of fear because I know that means growth. What I‚Äôve found is that I love fear because once I acknowledge it and push past it anyway, the rewards waiting on the other side make me want to chase after fear ten times over. Today, I‚Äôve now coached hundreds of ambitious corporate professionals to do the same‚Ä¶ scared of networking, scared of presenting, scared of trying‚Ä¶ they too have been able to enjoy the rewards waiting for them on the other side... dream jobs, dream collaborations, dream opportunities, and dream salaries. All from just trying something new! #inspire #cultivateyourcareer',\n",
       "  0),\n",
       " ('I talk to so many jobseekers that say something like this:\\n-\\n‚ÄúI‚Äôve been job searching for some time but I haven‚Äôt had any luck. Maybe I should change my profession.‚Äù\\n-\\nWhat they really mean is they‚Äôve been spending hours on their resume, applying online, and sitting back waiting for something to happen... for months. -\\nDon‚Äôt be fooled by applying online. It may feel like you‚Äôre being productive but it is the biggest time waster yet! -\\nYou‚Äôre not hearing anything back because you‚Äôre being passive. -\\nWhile you‚Äôre waiting, a less qualified candidate is messaging the hiring manager directly and asking for an interview. -\\nIt‚Äôs like people waiting in the general admission line at da club... you see that person walking up directly to the bouncer and getting in immediately despite you waiting there for 45 minutes plus and counting. -\\n‚ÄúConventional guys and girls finish last.‚Äù - Emily -\\nI challenge all of my clients to challenge conventional wisdom and do something different from what every other career coach is teaching. üò¨ because I promise you it works!',\n",
       "  0),\n",
       " ('‚ÄúThe unknown carries tremendous opportunities, knowledge, potential, and rewards. Step into it often.‚Äù - Gikandi üå±',\n",
       "  0),\n",
       " ('Aloha! Whenever I travel I am just amazed and in awe at what is created around us. From beautiful nature and all of the variety of plants and animals. To the no two sunsets or skies are the same. To the surfboards that people freaking ride waves on. To the perfect cup that holds my latte. We live in such an inspiring and magnificent world! ‚Ä¢\\nI am in my happy place! The state of being and feeling free, calm, at peace, and completely confident and in control of my future. ‚Ä¢\\nOne of my favorite activities has just been sitting next to the ocean and journaling, reading, and meditating. What‚Äôs amazing is all of this is FREE! Grateful for access to uplifting scenery and letting my imagination run wild. üå±\\n‚Ä¢\\n‚¨áÔ∏èWhat do you do for fun on an üå¥ island? ‚¨áÔ∏è',\n",
       "  0),\n",
       " ('I‚Äôve been reflecting on my 2018 and planning my 2019. üå± my mantra this year is going to be ‚ÄúTRUST and THRIVE!‚Äù Trust in myself, the universe/God and thrive to serve as many corporate professionals I can. üíï',\n",
       "  0),\n",
       " (\"I'm excited to announce that I am partnering with @netflix to tell stories on a global platform. So proud to join the ranks of digital content moguls as the first trans woman to call her own creative shots at a major content company. ‚ÄúAs a best-selling author, producer and director, Janet Mock has demonstrated she knows how to bring her vision to thrilling, vivid life,‚Äù says Cindy Holland, Netflix VP of original content. ‚ÄúShe‚Äôs a groundbreaker and creative force who we think will fit right in here at Netflix.‚Äù My mentor, boss and friend @mrrpmurphy told @variety, ‚ÄúUpon my first meeting with Janet, I knew she was a star and had the stuff moguls are made of. Being able to watch her grow, first as a writer, then a producer and now an acclaimed director who‚Äôs helmed four episodes for my productions, has been a gift. I am honored to be her mentor and friend, am grateful she‚Äôll be joining me to write and direct on ‚ÄòHollywood,‚Äô and am so excited to see what she creates at Netflix. Janet is a cultural force, and the world needs her stories.‚Äù\",\n",
       "  1),\n",
       " ('When you get that ‚Äúyou got RENEWED for SEASON 3‚Äù call!!! Thank you @fxnetworks for giving us a home, for believing in us, for equipping our cast and crew with the resources to make history and move folks to think and do different. Thank you to our loyal viewers for showing up for us and helping us make record numbers for our season 2 premiere (it was our MOST WATCHED EPISODE!) We know you can spend your time anywhere and the fact that you spend it with us means the world to us. We love you!!!! #poseFX',\n",
       "  1),\n",
       " ('Heartbroken and enraged from the harsh reality that Layleen Polanco, who was incarcerated on Rikers Island, was found dead in her cell on Friday. Folks are gathering Monday in NYC and collecting donations to help lay her to rest. Links are in my stories. We need to show up for trans women of color. We need to #CloseRikers. We need to check and challenge any one seeking to police our bodies, genders and identities. Rest in power, Layleen. #closerikersnow #twoc #layleenpolanco #justiceforlayleen #girlslikeus üì∑ @vrye',\n",
       "  0),\n",
       " ('G U C C I', 1),\n",
       " ('Thank you to each and every @televisionacad member for showing up to watch a FYC screening of my directorial debut LOVE IS THE MESSAGE (Pose season 1, episode 6). Thank you to our incomparable cast, writers and producers for making this hour of television possible. It would be a dream to be nominated as a director and a writer ‚Äî the first trans woman of color to do so! ‚Äî and I feel that drama series EMMY nomination for @poseonfx in my bones. Oh, and don‚Äôt forget that @mjrodriguez7 deserves all the üèÜ for carrying our show on her back ‚Äî where would any of us be without Mother Blanca?! Here we stand, together, for your consideration for this year‚Äôs Emmy Awards. #posefx #emmys #fyc',\n",
       "  0),\n",
       " ('The writers room of #PoseFX (missing Ryan and Bradley!) accepting our @televisionacad Honors üèÜ for @poseonfx: ‚ÄúFor its vivid portrayal of a unique subculture, and depiction of such topical issues as LGBTQ prejudice and acceptance, Pose is a worthy recipient.‚Äù',\n",
       "  0),\n",
       " ('V A L E N T I N O', 1),\n",
       " ('C A R O L I N A H E R R E R A', 1),\n",
       " ('I got ready for the #metgala with @elleusa rolling their üé•. It‚Äôs a ki. Get into it!',\n",
       "  1)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_pred_comments = [(comment, promo) for comment, promo in zip(false_predictions['comment'], false_predictions['promo'])]\n",
    "false_pred_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "79/248"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
