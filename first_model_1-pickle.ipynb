{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "from nltk import word_tokenize\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>age</th>\n",
       "      <th>comment</th>\n",
       "      <th>likes/views</th>\n",
       "      <th>link</th>\n",
       "      <th>promo</th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2 DAYS AGO</td>\n",
       "      <td>found my new favorite park!</td>\n",
       "      <td>405,059 likes</td>\n",
       "      <td>/BytNlrQhRx8/</td>\n",
       "      <td>0</td>\n",
       "      <td>chrissyteigen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3 DAYS AGO</td>\n",
       "      <td>Happy bebe!</td>\n",
       "      <td>1,739,218</td>\n",
       "      <td>/Byqz8uZh73s/</td>\n",
       "      <td>0</td>\n",
       "      <td>chrissyteigen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5 DAYS AGO</td>\n",
       "      <td>coated in a paste of fresh garlic and filled w...</td>\n",
       "      <td>2,931,603</td>\n",
       "      <td>/BymErW1B9eL/</td>\n",
       "      <td>0</td>\n",
       "      <td>chrissyteigen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5 DAYS AGO</td>\n",
       "      <td>this kid</td>\n",
       "      <td>371,095 likes</td>\n",
       "      <td>/Byl-aHjBXFX/</td>\n",
       "      <td>0</td>\n",
       "      <td>chrissyteigen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>JUNE 8</td>\n",
       "      <td>home tomorrow üò©</td>\n",
       "      <td>859,039 likes</td>\n",
       "      <td>/ByduG0BB_A3/</td>\n",
       "      <td>0</td>\n",
       "      <td>chrissyteigen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0         age                                            comment  \\\n",
       "0           0  2 DAYS AGO                        found my new favorite park!   \n",
       "1           1  3 DAYS AGO                                        Happy bebe!   \n",
       "2           2  5 DAYS AGO  coated in a paste of fresh garlic and filled w...   \n",
       "3           3  5 DAYS AGO                                           this kid   \n",
       "4           4      JUNE 8                                    home tomorrow üò©   \n",
       "\n",
       "     likes/views           link  promo           user  \n",
       "0  405,059 likes  /BytNlrQhRx8/      0  chrissyteigen  \n",
       "1      1,739,218  /Byqz8uZh73s/      0  chrissyteigen  \n",
       "2      2,931,603  /BymErW1B9eL/      0  chrissyteigen  \n",
       "3  371,095 likes  /Byl-aHjBXFX/      0  chrissyteigen  \n",
       "4  859,039 likes  /ByduG0BB_A3/      0  chrissyteigen  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_df = pd.read_csv('data/posts.csv')\n",
    "posts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1065, 7)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = posts_df.promo\n",
    "data = posts_df['comment'].map(word_tokenize).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8994 unique tokens in our dataset.\n"
     ]
    }
   ],
   "source": [
    "total_vocabulary = set(word for comment in data for word in comment)\n",
    "print(\"There are {} unique tokens in our dataset.\".format(len(total_vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = {}\n",
    "with open('data/glove_data/glove.6B.50d.txt', 'rb') as f:\n",
    "    for line in f:\n",
    "        parts = line.split()\n",
    "        word = parts[0].decode('utf-8')\n",
    "        if word in total_vocabulary:\n",
    "            vector = np.array(parts[1:], dtype=np.float32)\n",
    "            glove[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.65575 ,  0.45659 , -0.16748 , -0.58345 , -0.23073 , -0.78348 ,\n",
       "       -0.23166 , -0.022452, -0.57968 ,  0.526   , -0.2214  ,  0.17614 ,\n",
       "        0.46513 ,  0.79142 ,  0.017403,  1.0879  ,  0.24418 ,  0.27523 ,\n",
       "       -0.26452 , -1.0389  ,  0.014045,  0.68459 ,  0.98151 ,  0.21561 ,\n",
       "        0.36278 , -0.51819 , -0.40552 ,  1.349   ,  1.5399  ,  0.60541 ,\n",
       "        2.6604  ,  0.074535, -0.076292,  0.12501 , -0.026268,  0.16843 ,\n",
       "       -0.41844 ,  0.44505 ,  0.25033 , -1.1557  ,  0.24575 ,  0.41847 ,\n",
       "       -0.10633 , -0.28433 ,  0.51215 ,  0.51371 ,  0.53004 , -0.889   ,\n",
       "        0.054744,  0.78793 ], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove['cool']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Mean Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class W2vVectorizer(object):\n",
    "    \n",
    "    def __init__(self, w2v):\n",
    "        # takes in a dictionary of words and vectors as input\n",
    "        self.w2v = w2v\n",
    "        if len(w2v) == 0:\n",
    "            self.dimensions = 0\n",
    "        else:\n",
    "            self.dimensions = len(w2v[next(iter(glove))])\n",
    "    \n",
    "    # Note from Mike: Even though it doesn't do anything, it's required that this object implement a fit method or else\n",
    "    # It can't be used in a sklearn Pipeline. \n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "            \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.w2v[w] for w in words if w in self.w2v]\n",
    "                   or [np.zeros(self.dimensions)], axis=0) for words in X])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rf =  Pipeline([(\"Word2Vec Vectorizer\", W2vVectorizer(glove)),\n",
    "              (\"Random Forest\", RandomForestClassifier(n_estimators=100, verbose=True))])\n",
    "svc = Pipeline([(\"Word2Vec Vectorizer\", W2vVectorizer(glove)),\n",
    "                ('Support Vector Machine', SVC())])\n",
    "lr = Pipeline([(\"Word2Vec Vectorizer\", W2vVectorizer(glove)),\n",
    "              ('Logistic Regression', LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [('Random Forest', rf),\n",
    "          (\"Support Vector Machine\", svc),\n",
    "          (\"Logistic Regression\", lr)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "/Users/sherzyang/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/sherzyang/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n",
      "/Users/sherzyang/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/sherzyang/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "scores = [(name, cross_val_score(model, data, target, cv=2).mean()) for name, model, in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Random Forest', 0.6281722128962179),\n",
       " ('Support Vector Machine', 0.5624321121753728),\n",
       " ('Logistic Regression', 0.5877851288634344)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Learning with word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding\n",
    "from tensorflow.keras.layers import Dropout, Activation, Bidirectional, GlobalMaxPool1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from tensorflow.keras.preprocessing import text, sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.get_dummies(target).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer(num_words=20000)\n",
    "tokenizer.fit_on_texts(list(posts_df.comment))\n",
    "list_tokenized_headlines = tokenizer.texts_to_sequences(posts_df.comment)\n",
    "X_t = sequence.pad_sequences(list_tokenized_headlines, maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(tokenizer, open('instagram_flask/tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/sherzyang/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/sherzyang/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 128\n",
    "input_ = Input(shape=(100,))\n",
    "x = Embedding(20000, embedding_size)(input_)\n",
    "x = Bidirectional(LSTM(25, return_sequences=True))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dense(50, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "# There are 2 different possible classes, so we use 2 neurons in our output layer\n",
    "x = Dense(2, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=input_, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 100, 128)          2560000   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 100, 50)           30800     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 2,593,452\n",
      "Trainable params: 2,593,452\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 958 samples, validate on 107 samples\n",
      "WARNING:tensorflow:From /Users/sherzyang/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /Users/sherzyang/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/2\n",
      "958/958 [==============================] - 5s 5ms/sample - loss: 0.6817 - acc: 0.5511 - val_loss: 0.6597 - val_acc: 0.6075\n",
      "Epoch 2/2\n",
      "958/958 [==============================] - 3s 4ms/sample - loss: 0.6405 - acc: 0.6545 - val_loss: 0.6203 - val_acc: 0.7477\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a3f5771d0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_t, y, epochs=2, batch_size=32, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 958 samples, validate on 107 samples\n",
      "Epoch 1/10\n",
      "958/958 [==============================] - 3s 4ms/sample - loss: 0.4723 - acc: 0.8267 - val_loss: 0.5472 - val_acc: 0.7103\n",
      "Epoch 2/10\n",
      "958/958 [==============================] - 3s 4ms/sample - loss: 0.1812 - acc: 0.9478 - val_loss: 0.6441 - val_acc: 0.7383\n",
      "Epoch 3/10\n",
      "958/958 [==============================] - 3s 4ms/sample - loss: 0.0795 - acc: 0.9843 - val_loss: 0.6516 - val_acc: 0.7757\n",
      "Epoch 4/10\n",
      "958/958 [==============================] - 3s 4ms/sample - loss: 0.0368 - acc: 0.9916 - val_loss: 0.8280 - val_acc: 0.7850\n",
      "Epoch 5/10\n",
      "958/958 [==============================] - 3s 4ms/sample - loss: 0.0156 - acc: 0.9969 - val_loss: 0.9118 - val_acc: 0.7757\n",
      "Epoch 6/10\n",
      "958/958 [==============================] - 3s 4ms/sample - loss: 0.0083 - acc: 0.9990 - val_loss: 1.1302 - val_acc: 0.7570\n",
      "Epoch 7/10\n",
      "958/958 [==============================] - 3s 4ms/sample - loss: 0.0064 - acc: 0.9990 - val_loss: 1.1663 - val_acc: 0.7664\n",
      "Epoch 8/10\n",
      "958/958 [==============================] - 3s 4ms/sample - loss: 0.0059 - acc: 0.9990 - val_loss: 1.1852 - val_acc: 0.7757\n",
      "Epoch 9/10\n",
      "958/958 [==============================] - 3s 4ms/sample - loss: 0.0029 - acc: 1.0000 - val_loss: 1.2758 - val_acc: 0.7664\n",
      "Epoch 10/10\n",
      "958/958 [==============================] - 3s 4ms/sample - loss: 0.0025 - acc: 1.0000 - val_loss: 1.3584 - val_acc: 0.7664\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a3f611c50>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_t, y, epochs=10, batch_size=32, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "test_df = pickle.load(open('data/testing_posts.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tokenized_headlines = tokenizer.texts_to_sequences(test_df['comment'].map(word_tokenize).values)\n",
    "testing_X = sequence.pad_sequences(list_tokenized_headlines, maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_y = pd.get_dummies(test_df['promo']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248/248 [==============================] - 0s 719us/sample - loss: 1.7485 - acc: 0.7016\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.7484555628991896, 0.7016129]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(testing_X, testing_y, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(testing_X).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>promo</th>\n",
       "      <th>pred_not_promo</th>\n",
       "      <th>pred_promo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Summer is a great time to freshen up the kids‚Äô...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>They all look up to you so much @chipgaines Yo...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Happy Father's Day to the plant daddy of all p...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Y‚Äôall really stepped up to the #ChipInChalleng...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>One more push y‚Äôall! We have til midnight CT t...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  promo  pred_not_promo  \\\n",
       "0  Summer is a great time to freshen up the kids‚Äô...      1             0.0   \n",
       "1  They all look up to you so much @chipgaines Yo...      0             1.0   \n",
       "2  Happy Father's Day to the plant daddy of all p...      0             1.0   \n",
       "3  Y‚Äôall really stepped up to the #ChipInChalleng...      0             1.0   \n",
       "4  One more push y‚Äôall! We have til midnight CT t...      1             0.0   \n",
       "\n",
       "   pred_promo  \n",
       "0         1.0  \n",
       "1         0.0  \n",
       "2         0.0  \n",
       "3         0.0  \n",
       "4         1.0  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df = pd.DataFrame(predictions, columns=['pred_not_promo', 'pred_promo'])\n",
    "simple_test = test_df[['comment', 'promo']]\n",
    "combined_posts_df = pd.concat((simple_test, predictions_df), axis=1)\n",
    "combined_posts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predicts = []\n",
    "for row in range(len(combined_posts_df)):\n",
    "    pred_promo = combined_posts_df['pred_promo'][row]\n",
    "    pred_not_promo = combined_posts_df['pred_not_promo'][row]\n",
    "    if pred_promo > pred_not_promo:\n",
    "        model_predicts.append(1)\n",
    "    else:\n",
    "        model_predicts.append(0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>promo</th>\n",
       "      <th>pred_not_promo</th>\n",
       "      <th>pred_promo</th>\n",
       "      <th>model_predicts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Summer is a great time to freshen up the kids‚Äô...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>They all look up to you so much @chipgaines Yo...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Happy Father's Day to the plant daddy of all p...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Y‚Äôall really stepped up to the #ChipInChalleng...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>One more push y‚Äôall! We have til midnight CT t...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  promo  pred_not_promo  \\\n",
       "0  Summer is a great time to freshen up the kids‚Äô...      1             0.0   \n",
       "1  They all look up to you so much @chipgaines Yo...      0             1.0   \n",
       "2  Happy Father's Day to the plant daddy of all p...      0             1.0   \n",
       "3  Y‚Äôall really stepped up to the #ChipInChalleng...      0             1.0   \n",
       "4  One more push y‚Äôall! We have til midnight CT t...      1             0.0   \n",
       "\n",
       "   pred_promo  model_predicts  \n",
       "0         1.0               1  \n",
       "1         0.0               0  \n",
       "2         0.0               0  \n",
       "3         0.0               0  \n",
       "4         1.0               1  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_posts_df['model_predicts'] = model_predicts\n",
    "combined_posts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_pred = []\n",
    "for row in range(len(combined_posts_df)):\n",
    "    true_class = combined_posts_df['promo'][row]\n",
    "    model_pred = combined_posts_df['model_predicts'][row]\n",
    "    if true_class == model_pred:\n",
    "        correct_pred.append(True)\n",
    "    else:\n",
    "        correct_pred.append(False)\n",
    "        \n",
    "correct_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>promo</th>\n",
       "      <th>pred_not_promo</th>\n",
       "      <th>pred_promo</th>\n",
       "      <th>model_predicts</th>\n",
       "      <th>correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Summer is a great time to freshen up the kids‚Äô...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>They all look up to you so much @chipgaines Yo...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Happy Father's Day to the plant daddy of all p...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Y‚Äôall really stepped up to the #ChipInChalleng...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>One more push y‚Äôall! We have til midnight CT t...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>After months of developing and finalizing reci...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>When @chipgaines and I got the chance to meet ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Course complete! School looks good on you @chi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What we witnessed this week at @harvardhbs was...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.96</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>16 years and it feels like we're just getting ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  promo  pred_not_promo  \\\n",
       "0  Summer is a great time to freshen up the kids‚Äô...      1            0.00   \n",
       "1  They all look up to you so much @chipgaines Yo...      0            1.00   \n",
       "2  Happy Father's Day to the plant daddy of all p...      0            1.00   \n",
       "3  Y‚Äôall really stepped up to the #ChipInChalleng...      0            1.00   \n",
       "4  One more push y‚Äôall! We have til midnight CT t...      1            0.00   \n",
       "5  After months of developing and finalizing reci...      1            0.00   \n",
       "6  When @chipgaines and I got the chance to meet ...      1            1.00   \n",
       "7  Course complete! School looks good on you @chi...      0            0.98   \n",
       "8  What we witnessed this week at @harvardhbs was...      0            0.04   \n",
       "9  16 years and it feels like we're just getting ...      0            0.68   \n",
       "\n",
       "   pred_promo  model_predicts  correct  \n",
       "0        1.00               1     True  \n",
       "1        0.00               0     True  \n",
       "2        0.00               0     True  \n",
       "3        0.00               0     True  \n",
       "4        1.00               1     True  \n",
       "5        1.00               1     True  \n",
       "6        0.00               0    False  \n",
       "7        0.02               0     True  \n",
       "8        0.96               1    False  \n",
       "9        0.32               0     True  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_posts_df['correct'] = correct_pred\n",
    "combined_posts_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "false_predictions = combined_posts_df.loc[combined_posts_df['correct'] == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('When @chipgaines and I got the chance to meet some of the kids at @stjude a couple of years ago, we walked away completely changed. These kids were so full of joy and hope‚ÄîI‚Äôll never forget it. That year, you all helped us raise $230,000 for those kids, and we want to do it again‚Äîbut go even bigger! We‚Äôve pulled a team together to help us with what we‚Äôre calling the #ChipInChallenge, and we hope you‚Äôll be a part of helping us change lives. Head over to the link in profile to find out how you can help.',\n",
       "  1),\n",
       " (\"What we witnessed this week at @harvardhbs was human beings at their very finest. 85 people from all around the world, from all different walks of lives, with different cultural backgrounds, religions, political beliefs and professions. We learned from one another and spoke into each other's lives, sometimes challenging but always valuing every perspective. Not a single one of us were alike, and yet by doing something as simple as listening to and respecting one another, we were challenged and made better. Thankful for this opportunity @anitaelberse ‚Äì your leadership and your passion caused grown, busy adults to think with curiosity about our own businesses and about the world around us. Oh and one more thing... I know I might be biased but I'd like to officially cast my vote for Chip as CLASS PREZ. #ChipForPrez (Photo by @evephoto ) #bems\",\n",
       "  0),\n",
       " ('She is the most intentional, loyal and fierce woman I know. She is small but mighty and she opens her arms wide for so many. Thank you mom for your extravagant love. I love you‚ù§Ô∏è \"She is clothed with strength and dignity and laughs at the days to come. She speaks with wisdom, and faithful instruction is on her tongue. She watches over the affairs of her household, and does not eat the bread of idleness. Her children arise and call her blessed. Many women do noble things, but you surpass them all.\" Prov 31',\n",
       "  0),\n",
       " (\"We just finished construction on this beauty and I'm loving all the storage and space in this home. Excited for the lovely family that is moving in next month! #welcomehome @magnoliarealtytexas\",\n",
       "  1),\n",
       " ('Race weekend is finally here and we are ready! To all the runners and families- Welcome to town! üôåüèΩ#silosdistrictmarathon @magnolia',\n",
       "  0),\n",
       " ('@ysl @anthonyvaccarello üì∏ @chriscolls', 1),\n",
       " ('Thanks for the funky shoot @gqaustralia had a great time wearing a colorful array of expensive clothing ;) üì∏ @carterbedloesmith / stylist @atvottero',\n",
       "  1),\n",
       " ('Thanks for the funky shoot @gqaustralia had a great time wearing a colorful array of expensive clothing ;) üì∏ @carterbedloesmith / stylist @atvottero',\n",
       "  1),\n",
       " ('Been messing around with the @redhydrogen camera. Super fun. Check out some of the pics I put on holowpix in 4V. It‚Äôs really spicy üå∂üòé',\n",
       "  0),\n",
       " ('Sorry I couldn‚Äôt make it to the @isntitromantic premier tonight guys. Been dealing with some pretty annoying health stuff the last couple days. Lucky I have the best girl in the world to represent for me! Thanks for the support babe! I hope everyone enjoys the movie! It‚Äôs a perfect Valentine‚Äôs Day flick so if ya ain‚Äôt got nothin better to do then go see it! Love to all!',\n",
       "  1),\n",
       " ('It‚Äôs been a heartbreaking few days. This is what‚Äôs left of my house. Love. Many people in Malibu and surrounding areas in California have lost their homes also and my heart goes out to everyone who was affected by these fires. I spent the day in Malibu yesterday and it was amazing to see the community pulling together to help each other out in any way they can. Malibu is a strong community and this event is only going to make it stronger. Thankful for the all the great local guys that helped keep smaller fires out around my property. I love u guys. I love you Malibu. Thank you to all the hero firefighters around California. It‚Äôs going to be a journey to rebuild. Stay strong all. To help/donate visit @malibufoundation and @happyhippiefdn',\n",
       "  1),\n",
       " ('Happy Halloween! Eat your heart out for the first ever ‚ÄúISN‚ÄôT IT ROMANTIC‚Äù trailer! Valentine‚Äôs Day 2019.',\n",
       "  1),\n",
       " ('I live for a queen who writes their elected officials üè≥Ô∏è\\u200düåà love the video & thanks for having us @taylorswift',\n",
       "  0),\n",
       " ('Making our way through this gorgeous tour w @kylejune comedians extraordinaries üé∂ @cher üè≥Ô∏è\\u200düåà #roadtobeijing',\n",
       "  1),\n",
       " ('Last June 4th, the Supreme Court ruled in favor of a cake shop owner who chose not to bake a wedding cake for a same-sex couple. On the one year anniversary of the reversal, I was honored to partner with @elysianbrewing to officiate a very special wedding. Many happy tears were shed as Megan @meganberrycreative & Haden @bart_arts tied the knot and we cut into six epic cakes alongside the court case couple, Charlie @charliewcraig & David @moofian2 . Cheers to marriage equality and happily ever after for everyone. #MarryUsJVN thanks so much @elysianbrewing for making such a beautiful and celebratory day #pride #ad ‚Äî A note on this ruling - it‚Äôs important to clarify this Supreme Court decision was narrow & does not establish precedent for LGBTQ discrimination in services nationally but rather it specifically reversed the ruling in this case for extenuating circumstances.',\n",
       "  1),\n",
       " ('Still can‚Äôt believe I got to meet these two fuquin queens who need to nominated for lots of Emmys üè≥Ô∏è\\u200düåàüí´@abbijacobson @ilana @televisionacad #foryourconsideration',\n",
       "  0),\n",
       " ('I‚Äôve been thinking about this a lot lately üè≥Ô∏è\\u200düåà Thanks @fransquishco for your time & talent read the full article @outmagazine',\n",
       "  1),\n",
       " ('Thank you so much Chicago üè≥Ô∏è\\u200düåà 2 Sold out shows @chicagotheatre ü§∏üèª\\u200d‚ôÄÔ∏èüò≠üò≠üôèüèªüôèüèª We hope your stomachs are sore from laughing so much @kylejune @jaboukie üí´üí´ Indianapolis can‚Äôt wait for our gorgeous SOLD OUT show tonite!!! üì∏ @andy.argyrakis',\n",
       "  1),\n",
       " ('It‚Äôs #nationalgunviolenceawarenessday & as @speakerpelosi wore orange when I interviewed her last year it‚Äôs the color of #gunviolenceawareness - While these hashtags and days of awareness are important & useful it‚Äôs even better to do something actionable now from this post. The gun lobby @nationalrifleassociation has a death grip on our government and they do so with money. To combat that follow & donate to @momsdemand , @everytown or look up your state legislatures & federal to find out their voting records on gun safety. Our lives have all been touched in someway by gun violence and let‚Äôs wake up & help the people fighting on the front lines. Tag reps fighting for gun safety like @replucymcbath #wearorange',\n",
       "  1),\n",
       " ('I‚Äôm so honored to announce I‚Äôve teamed up with @essie as their first non-female ambassador in celebration of Pride! For me, polish has always been a form of self-expression. Right now that means this mosaic rainbow mani moment. Wearing it proud! üè≥Ô∏è\\u200düåàüíï #essiePartner #essielove #LorealProud',\n",
       "  1),\n",
       " ('Thank you all so so much üè≥Ô∏è\\u200düåà Male Star or the year?? To all the people that voted thank you so much, so so grateful and proud of our team @queereye I just really honestly can not believe üí´üí´',\n",
       "  1),\n",
       " ('Happy Pride loves üíïüíô Time to celebrate with your LGBTQ fam & allies alike to celebrate the diversity that makes everyone stronger! üåàüåàüíï @queereye',\n",
       "  1),\n",
       " ('The folks @wilsongolf have me dialed-in for @TheMasters this week. Excited to get things going in Augusta! #WilsonStaff',\n",
       "  1),\n",
       " ('@pumagolf got my threads. Time to focus on my game now. It‚Äôs #TheMasters. Let‚Äôs go!',\n",
       "  1),\n",
       " ('I‚Äôm proud to partner with @CDWcorp and their #TechFore mission to help kids learn valuable life skills through access to golf and technology. #peoplewhogetIT',\n",
       "  1),\n",
       " ('Love wearing @volitionamerica this season. Nothing better than the red, white and blue- especially Blue! Rock Chalk!!',\n",
       "  1),\n",
       " ('Check out Inside the PGA Tour this week on Golf Channel.', 1),\n",
       " ('Well it‚Äôs official! Excited to be wearing @pumagolf this year and supporting @volitionameria and @foldsofhonor. I‚Äôll be looking good, feeling good and representing a great cause. #ResistOrdinary',\n",
       "  1),\n",
       " ('Don‚Äôt usually play golf on vacation but @playagrandeclub is so good I had to play a couple times. Luckily @gabby_woodland came to caddie and keep the music going',\n",
       "  1),\n",
       " ('Thank you Bellerive Country Club for the hospitality today. Excited to come back next month for the PGA Championship',\n",
       "  0),\n",
       " ('Little fine tuning with Butch before playing 3 weeks in a row', 1),\n",
       " ('Truly an incredible experience this week at the @wmphoenixopen. Thank you to the tournament for once again putting on one of the best events all year, the fans for creating an atmosphere that is unmatched, my partners @uagolf, @CDWcorp, @taylormadegolf, and @netjets for the endless support, and my family for waiting for me as I walked off 18.',\n",
       "  0),\n",
       " ('This pregnant lady is extra happy about 1) her strawberry ice cream and 2) her comfy striped @jcrew dress! Thanks @jcrew for having us at your Summer Soir√©e in the beautiful Elizabeth Street Garden gearing up for #nationalstripeday #jcrew',\n",
       "  1),\n",
       " ('The three of us üå∏üå∏üå∏ @tberolz is over on galmeetsglam.com today sharing his top picks for Father‚Äôs Day gift ideas! Head on over to the link in my profile for more #babyb #petershamnurseries #london #mylove #gmgcollection #gmgonme',\n",
       "  1),\n",
       " ('Picking up things for Baby B in the small charming town of Valldemossa. Love seeking out local shops and shopping small with @AmericanExpress to support small businesses at home and abroad. I can‚Äôt wait to see the little lady in the dress we got her! #ShopSmall #AmexAmbassador #AmexLife #gmgcollection #gmgonme',\n",
       "  1),\n",
       " ('Hi guys I just wanted to share with you my pre and post workout supplements from @myntrx üí™üèº The pre-workout has changed working out for me it gives me so much energy. The BCAA‚Äôs taste so yummy and are so hydrating. Go check out their page and give them a follow!!',\n",
       "  1),\n",
       " ('nothing is better than getting your hair done!!! thank you to @jillenefarrbeauty and @heather.formsalon for making me blonde and happy againüòçüòç',\n",
       "  1),\n",
       " ('After trying my sister @lindsarnold‚Äôs @luftbeds I loved it so much that Topher and I needed to get one for our new apt! A great night sleep is so important to my recovery and fitness routine and @Luftbeds gives me that! You saw my IG story over Presidents Day weekend about #Luft and they let me offer to you all an extension on these awesome deals! So, use discount code JensenQueen ($200 off)  or JensenKing ($300 Off). The mattresses are delivered right to your door! #luftbeds #livelifted #mattress #handmade #madeinusa #luftpartner',\n",
       "  1),\n",
       " ('I need @marisarosemph everyday for my makeupüòç also @aubreebellephotography you‚Äôre the best photographer!! ‚ù§Ô∏è',\n",
       "  1),\n",
       " ('@maglebys_catering is the best you guys!!! If you are looking for a caterer for the big day you need them! The food and desserts were everything that we wanted, and tasted so good! Look how cute the mini donuts areüòç The grilled cheese & tomato soup, and the famous Magleby‚Äôs chocolate cake as our wedding cake made Toph and I soooo happy!‚ù§Ô∏è',\n",
       "  1),\n",
       " ('GIVEAWAY CLOSED!!! @bamatinkerbelle you are the WINNER!!! I want to thank @calledtosurf for my bridesmaid dresses! I got to customize and make them which was such a fun process, and they are the best to work with. Also how pretty are the girls standing next to meüòç I partnered up with them and we are doing a GIVEAWAY up to 5 bridesmaid dresses!! TO ENTER:\\nLIKE THIS POST, FOLLOW @calledtosurf and @jensenarnold_, TAG 3 FRIENDS!! Winner will be chosen by Friday!! Good luckü§©',\n",
       "  1),\n",
       " ('Our wedding featured on @people üíò link in my bio :) ________________________________________\\n\\nI want to give a HUGE thank you to @pritchettbridal for my dress!! It was everything I wanted and more!! Thank you to @chandikatebeauty for doing my hair and make up for my bridals and wedding day. She is the best and made me feel so beautiful. Also thank you to @pronovias for the cutest fur wrap for my winter wedding!',\n",
       "  1),\n",
       " ('As I once said to @amtendler ‚Äúyou are an art project that is always evolving. I am like...a benign cartoon character.‚Äù This profile in @nylonmag captures the first part of that sentiment. Reading this just reminds me how amazing and lucky it is to watch this woman create every day. Lincoln bio.',\n",
       "  1),\n",
       " ('#tbt May 9 2013. I had the ring in my hand, she turned around, and before I could say ‚Äúwill you..‚Äù she said ‚Äúwait do you have sunscreen on?‚Äù She‚Äôs the fucking best.',\n",
       "  0),\n",
       " ('2015. ‚ÄúThe Devil and Petunia‚Äù also titled ‚ÄúThe Devil and Anthony Jeselnik.‚Äù Anthony‚Äôs new special FIRE IN THE MATERNITY WARD is perfect.\\nIt‚Äôs on @netflixisajoke.',\n",
       "  1),\n",
       " ('Wise Child, Simple Child, Wicked Child and Child Who Doesn‚Äôt Know How to Ask Questions, all roughly 36 years old, looking for and finding the afikoman. Congratulations @odtron!',\n",
       "  0),\n",
       " ('\\u202aMy Nana has served the homeless and needy of Lynn, MA at My Brother‚Äôs Table for decades. So I am doing a benefit for them in BOSTON MAY 6 with every Nanas favorite comic Pete Davidson. The link is in my bio. Tickets are more than usual but every dollar goes to a wonderful organization. And you‚Äôll make my Nana really happy.\\n\\nPoster by the great @marcusrussellprice',\n",
       "  1),\n",
       " ('Late Shows Added', 1),\n",
       " ('Walk, jog, run or even dance for a chance to win a trip to Manchester! Join @tecnomobileindia @racetomancity, powered by Stepathlon. Grab a partner and sign up now! (bit.ly/Shibani) #racetomancity #wintogether #mancity #healthylifestyle @ravi_krishnan1',\n",
       "  1),\n",
       " ('@zimmermann @sacaiofficial @burberry styled by @khyatibusa hair @reenadutta123 makeup @ritarathod786 #Repost @gqindia\\n„Éª„Éª„Éª\\n#GQBestDressed 100: Potent, savvy dressers in this trio: Dior rep @anuragty, @shibanidandekar and musician @zaedenmusic. #June2019 Photo: Manasi Sawant\\n\\n_________________________________________\\n\\n#BestDressed #StyleFlex #Fashion #ShibaniDandekar #Dior #AnuragTyagi #Zaeden #SahilSharma #Musician #GQShoot',\n",
       "  1),\n",
       " ('Miss you already coach! @drewnealpt 9 incredible days of strength training, functional and boxing from the gym to the park! Don‚Äôt get better than that! ü•äüå≥#bodybydrewneal @faroutakhtar RECORD!!!!! üòÇ',\n",
       "  0),\n",
       " ('CANDY FLOSS üëÖ thank you thank you thank you @shehlaakhan for this custom piece! styled by my rock @khyatibusa makeup by my girl @ashreyaa jewels by @outhousejewellery @curiocottagejewelry',\n",
       "  1),\n",
       " ('@faroutakhtar üç≠\\nthank you thank you @shehlaakhan styled by @khyatibusa\\nhair and makeup by @ashreyaa\\njewels by @outhousejewellery @curiocottagejewelry',\n",
       "  1),\n",
       " ('üç¨\\nDress by the incredible @shehlaakhan love you girl! styled by my girl @khyatibusa makeup by my lovely @ashreyaa üå∏ jewelry by @outhousejewellery\\n@curiocottagejewellery #thatbrowngirl',\n",
       "  1),\n",
       " ('#beachbum #thatbrowngirl\\nbody by @drewnealpt #bodybydrewneal\\nphoto by @faroutakhtar\\noutfit by @hm\\n#nofilter Monday‚Äôs!',\n",
       "  1),\n",
       " ('Imma be right here hanging out!\\n@khyatibusa @sashajairam @anishaachhabriamakeup @reenadutta123 @azima_toppo @aspiringshe\\nKeep following me on @helo.app for all the latest updates!',\n",
       "  1),\n",
       " ('Mid week ballin with #dmoney @monicadogra #thatbrowngirl üöÅ üôåüèΩ photo bombing swag @kizeesmack üì∑ @faroutakhtar shades by @rayban_india',\n",
       "  1),\n",
       " ('#PSGIRLS are the happiest @monicadogra @payalsinghal earrings on #thatbrowngirl by @outhousejewellery styled by @khyatibusa üåºüå∏ üì∑ @faroutakhtar üåü',\n",
       "  1),\n",
       " ('Serving some #SHEINXshibani on this fine Tuesday! @shein_in Best time working on this collab! üíê @tanvivoraphotography @theparag26 @anishaachhabriamakeup @reenadutta123 @azima_toppo @nidhijeswani',\n",
       "  1),\n",
       " ('If I had an office pup like this I would definitely love going into work everyday! üòù üì¢all job seekers: sign up for the free 5-day challenge and tag a friend who can use help landing their dream job!',\n",
       "  1),\n",
       " ('In 2016, I made it my mission to always feel and embrace feelings of fear because I know that means growth. What I‚Äôve found is that I love fear because once I acknowledge it and push past it anyway, the rewards waiting on the other side make me want to chase after fear ten times over. Today, I‚Äôve now coached hundreds of ambitious corporate professionals to do the same‚Ä¶ scared of networking, scared of presenting, scared of trying‚Ä¶ they too have been able to enjoy the rewards waiting for them on the other side... dream jobs, dream collaborations, dream opportunities, and dream salaries. All from just trying something new! #inspire #cultivateyourcareer',\n",
       "  0),\n",
       " ('Thank you for all of the birthday love üéÇüéàüéâ\\n„ÄÇ\\nAs I get older, I realize everything I could possibly want, I already have. „ÄÇ\\nThat‚Äôs why this year I want to give back to others who don‚Äôt even have basic needs met. „ÄÇ\\nI‚Äôve decided to make a birthday pledge with @charitywater so millions of people around the globe can have access to clean water and therefore better health, education... everything!\\n„ÄÇ\\nIf you‚Äôre able to donate or simply share the fundraising page (link in bio) it would be the best birthday gift üéÅ Together we can change this world and make it a better place than we found it. „ÄÇ\\nM w a h !',\n",
       "  1),\n",
       " ('I wrote an article this morning on how to find your life purpose. If you asked my dad what his was, he would have stated, ‚ÄúI don‚Äôt know.‚Äù -\\nWe were all put on this planet for a very specific reason and we all have a unique purpose. -\\nMy father‚Äôs recent passing illuminated a very clear purpose: to be the lifesaver that helps everyone who is lucky enough to cross his path. -\\nIn this article (link in bio) I‚Äôve included a powerful exercise in helping you craft your own personal mission statement. Because everyone needs one and has one! üôåüèº\\n-\\nThank you, dad, for everything. Rest In Peace. 03.19.2019. üôèüèºüíï',\n",
       "  1),\n",
       " ('I talk to so many jobseekers that say something like this:\\n-\\n‚ÄúI‚Äôve been job searching for some time but I haven‚Äôt had any luck. Maybe I should change my profession.‚Äù\\n-\\nWhat they really mean is they‚Äôve been spending hours on their resume, applying online, and sitting back waiting for something to happen... for months. -\\nDon‚Äôt be fooled by applying online. It may feel like you‚Äôre being productive but it is the biggest time waster yet! -\\nYou‚Äôre not hearing anything back because you‚Äôre being passive. -\\nWhile you‚Äôre waiting, a less qualified candidate is messaging the hiring manager directly and asking for an interview. -\\nIt‚Äôs like people waiting in the general admission line at da club... you see that person walking up directly to the bouncer and getting in immediately despite you waiting there for 45 minutes plus and counting. -\\n‚ÄúConventional guys and girls finish last.‚Äù - Emily -\\nI challenge all of my clients to challenge conventional wisdom and do something different from what every other career coach is teaching. üò¨ because I promise you it works!',\n",
       "  0),\n",
       " ('‚ÄúThe unknown carries tremendous opportunities, knowledge, potential, and rewards. Step into it often.‚Äù - Gikandi üå±',\n",
       "  0),\n",
       " ('Aloha! Whenever I travel I am just amazed and in awe at what is created around us. From beautiful nature and all of the variety of plants and animals. To the no two sunsets or skies are the same. To the surfboards that people freaking ride waves on. To the perfect cup that holds my latte. We live in such an inspiring and magnificent world! ‚Ä¢\\nI am in my happy place! The state of being and feeling free, calm, at peace, and completely confident and in control of my future. ‚Ä¢\\nOne of my favorite activities has just been sitting next to the ocean and journaling, reading, and meditating. What‚Äôs amazing is all of this is FREE! Grateful for access to uplifting scenery and letting my imagination run wild. üå±\\n‚Ä¢\\n‚¨áÔ∏èWhat do you do for fun on an üå¥ island? ‚¨áÔ∏è',\n",
       "  0),\n",
       " ('Oops I am a day late... happy new year!!!! ü•Çü•Çü•Ç Chris and I spent ours with in n out, watching Wonder, and creating vision boards. It was an introverts‚Äô dream. üòç 364 days left ... let‚Äôs create magic!!',\n",
       "  0),\n",
       " ('When you get that ‚Äúyou got RENEWED for SEASON 3‚Äù call!!! Thank you @fxnetworks for giving us a home, for believing in us, for equipping our cast and crew with the resources to make history and move folks to think and do different. Thank you to our loyal viewers for showing up for us and helping us make record numbers for our season 2 premiere (it was our MOST WATCHED EPISODE!) We know you can spend your time anywhere and the fact that you spend it with us means the world to us. We love you!!!! #poseFX',\n",
       "  1),\n",
       " ('Heartbroken and enraged from the harsh reality that Layleen Polanco, who was incarcerated on Rikers Island, was found dead in her cell on Friday. Folks are gathering Monday in NYC and collecting donations to help lay her to rest. Links are in my stories. We need to show up for trans women of color. We need to #CloseRikers. We need to check and challenge any one seeking to police our bodies, genders and identities. Rest in power, Layleen. #closerikersnow #twoc #layleenpolanco #justiceforlayleen #girlslikeus üì∑ @vrye',\n",
       "  0),\n",
       " ('G U C C I', 1),\n",
       " ('V A L E N T I N O', 1),\n",
       " ('Wore my bossiest pants to speak at the @wsj‚Äôs FUTURE OF EVERYTHING Festival where I discussed my path as a storyteller through writing and directing, the power of POSE, and how recruiting, hiring and equipping marginalized folks to tell their own stories makes for the most authentic, dynamic and impactful storytelling. My hope is that the practices we‚Äôve implemented on POSE becomes industry standard. #wsjfuture',\n",
       "  0),\n",
       " ('C A R O L I N A H E R R E R A', 1),\n",
       " ('I got ready for the #metgala with @elleusa rolling their üé•. It‚Äôs a ki. Get into it!',\n",
       "  1)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_pred_comments = [(comment, promo) for comment, promo in zip(false_predictions['comment'], false_predictions['promo'])]\n",
    "false_pred_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+git://github.com/keras-team/keras.git\n",
      "  Cloning git://github.com/keras-team/keras.git to /private/var/folders/_y/3n9nd3td38132rxy737hzs_m0000gn/T/pip-req-build-ygqhsc2b\n",
      "  Running command git clone -q git://github.com/keras-team/keras.git /private/var/folders/_y/3n9nd3td38132rxy737hzs_m0000gn/T/pip-req-build-ygqhsc2b\n",
      "Building wheels for collected packages: Keras\n",
      "  Building wheel for Keras (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /private/var/folders/_y/3n9nd3td38132rxy737hzs_m0000gn/T/pip-ephem-wheel-cache-foh0oa3e/wheels/e6/02/ad/5e8e1a5824af71082e2260fe7e2eaa1b745c34706e6ff0a14b\n",
      "Successfully built Keras\n",
      "Installing collected packages: Keras\n",
      "  Found existing installation: Keras 2.2.4\n",
      "    Uninstalling Keras-2.2.4:\n",
      "      Successfully uninstalled Keras-2.2.4\n",
      "Successfully installed Keras-2.2.4\n"
     ]
    }
   ],
   "source": [
    "! pip install git+git://github.com/keras-team/keras.git --upgrade --no-deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_promo = posts_df.loc[posts_df['promo'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_promo = posts_df.loc[posts_df['promo'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_promo.to_csv('data/is_promo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_promo.to_csv('data/not_promo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Model Using Pickle\n",
    "import pickle\n",
    "model.save(\"instagram_flask/app_model.h5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
